<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=dark data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><title>Argument against recent research on compressing text tokens using images in MLLMs &#183; Barnaby's Blog</title>
<meta name=title content="Argument against recent research on compressing text tokens using images in MLLMs &#183; Barnaby's Blog"><meta name=description content="An argument against recent research, such as Deepseek's OCR paper, that suggests reducing the number of tokens used by text by rendering it first as an image and using a visual encoder."><meta name=keywords content="AI,"><link rel=canonical href=https://barnabywreford.co.uk/notebook/on_visual_text_context_compression/><link type=text/css rel=stylesheet href=/css/main.bundle.min.4ed2220b06454610a8a494fc6a9246b704e91bf845506e882a226f03c88b69098edc150c8f38844da43e356b1c3891161cd1b0ca131847ee658578ce4983ac59.css integrity="sha512-TtIiCwZFRhCopJT8apJGtwTpG/hFUG6IKiJvA8iLaQmO3BUMjziETaQ+NWscOJEWHNGwyhMYR+5lhXjOSYOsWQ=="><script type=text/javascript src=/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/js/main.bundle.min.3fc65be8df47ebc01260b43d7620a42d41769de3b3933d9263b4ec813e87828f9a851f1127ffff8ec45a65cd326726cf1f1ac529f341039c65f3f887e8fd6051.js integrity="sha512-P8Zb6N9H68ASYLQ9diCkLUF2neOzkz2SY7TsgT6Hgo+ahR8RJ///jsRaZc0yZybPHxrFKfNBA5xl8/iH6P1gUQ==" data-copy data-copied></script><script src=/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><meta property="og:title" content="Argument against recent research on compressing text tokens using images in MLLMs"><meta property="og:description" content="An argument against recent research, such as Deepseek's OCR paper, that suggests reducing the number of tokens used by text by rendering it first as an image and using a visual encoder."><meta property="og:type" content="article"><meta property="og:url" content="https://barnabywreford.co.uk/notebook/on_visual_text_context_compression/"><meta property="article:section" content="notebook"><meta property="article:published_time" content="2025-10-23T00:00:00+00:00"><meta property="article:modified_time" content="2025-10-23T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Argument against recent research on compressing text tokens using images in MLLMs"><meta name=twitter:description content="An argument against recent research, such as Deepseek's OCR paper, that suggests reducing the number of tokens used by text by rendering it first as an image and using a visual encoder."><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Notebook","name":"Argument against recent research on compressing text tokens using images in MLLMs","headline":"Argument against recent research on compressing text tokens using images in MLLMs","description":"An argument against recent research, such as Deepseek\u0027s OCR paper, that suggests reducing the number of tokens used by text by rendering it first as an image and using a visual encoder.","abstract":"TLDR: if you have a text input and want to compress the number of tokens fed to your decoder, use a text-based encoder, don\u0026rsquo;t render an image and use a visual encoder.","inLanguage":"en","url":"https:\/\/barnabywreford.co.uk\/notebook\/on_visual_text_context_compression\/","author":{"@type":"Person","name":"Barnaby Wreford"},"copyrightYear":"2025","dateCreated":"2025-10-23T00:00:00\u002b00:00","datePublished":"2025-10-23T00:00:00\u002b00:00","dateModified":"2025-10-23T00:00:00\u002b00:00","keywords":["AI"],"mainEntityOfPage":"true","wordCount":"3982"}]</script><meta name=author content="Barnaby Wreford"><link href=mailto:bwreford.contact@gmail.com rel=me><link href=https://github.com/barnaby-wreford rel=me><link href=https://www.linkedin.com/in/barnaby-wreford-8a9241318 rel=me><script src=/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-YVTB4ZDJ50"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-YVTB4ZDJ50")</script><meta name=theme-color></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start gap-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ class="text-base font-medium text-gray-500 hover:text-gray-900">Barnaby&rsquo;s Blog</a></nav><nav class="hidden md:flex items-center gap-x-5 md:ml-12 h-12"><a href=/notebook/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=Notebook>Notebook</p></a><a href=/problems/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=Problems>Problems</p></a><a href=/projects/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=Projects>Projects</p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center gap-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400 ltr:mr-1 rtl:ml-1"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/notebook/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=Notebook>Notebook</p></a></li><li class=mt-1><a href=/problems/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=Problems>Problems</p></a></li><li class=mt-1><a href=/projects/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=Projects>Projects</p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><header id=single_header class="mt-5 max-w-prose"><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">Argument against recent research on compressing text tokens using images in MLLMs</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2025-10-23T00:00:00+00:00>23 October 2025</time><span class="px-2 text-primary-500">&#183;</span><span>3982 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">19 mins</span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="Barnaby Wreford" src=/profile_hu68471ce5249335158e00a42614510882_266732_192x192_fill_box_center_3.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">Barnaby Wreford</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=mailto:bwreford.contact@gmail.com target=_blank aria-label=Email rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/barnaby-wreford target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://www.linkedin.com/in/barnaby-wreford-8a9241318 target=_blank aria-label=Linkedin rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><a href=#introduction-and-context>Introduction and Context</a></li><li><a href=#high-level-overview-of-how-it-works>High level overview of how it works</a><ul><li><a href=#how-llms-see-text-and-images>How LLMs see text and images</a></li><li><a href=#what-is-compression-and-how-does-it-apply-here>What is compression, and how does it apply here?</a></li></ul></li><li><a href=#comments-on-recent-papers>Comments on recent papers</a><ul><li><a href=#deepseek-ocr-contexts-optical-compression>DeepSeek-OCR: Contexts Optical Compression</a></li><li><a href=#see-the-text-from-tokenization-to-visual-reading>See The Text: From Tokenization to Visual Reading</a></li><li><a href=#text-or-pixels-it-takes-half-on-the-token-efficiency-of-visual-text-inputs>Text or Pixels? It Takes Half: On The Token Efficiency Of Visual Text Inputs</a></li><li><a href=#bonus-andrej-karpathy-tweet>bonus: Andrej Karpathy tweet</a></li></ul></li><li><a href=#main-arguments>Main Arguments</a><ul><li><ul><li><a href=#pixel-based-representations-of-text-are-worse>pixel-based representations of text are worse</a></li><li><a href=#over-eagerness-to-make-analogies-to-biological-brains>Over-eagerness to make analogies to biological brains</a></li><li><a href=#vision-isnt-even-how-humans-understand-language-fundamentally>vision isn&rsquo;t even how humans understand language (fundamentally)</a></li><li><a href=#why-the-comparison-between-visual-and-text-tokens-isnt-fair>Why the comparison between visual and text tokens isn&rsquo;t fair</a></li><li><a href=#why-we-should-consider-adding-the-text-encoder-back-in-some-form>Why we should consider adding the text encoder back (in some form)</a></li></ul></li></ul></li><li><a href=#conclusion>Conclusion</a></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><a href=#introduction-and-context>Introduction and Context</a></li><li><a href=#high-level-overview-of-how-it-works>High level overview of how it works</a><ul><li><a href=#how-llms-see-text-and-images>How LLMs see text and images</a></li><li><a href=#what-is-compression-and-how-does-it-apply-here>What is compression, and how does it apply here?</a></li></ul></li><li><a href=#comments-on-recent-papers>Comments on recent papers</a><ul><li><a href=#deepseek-ocr-contexts-optical-compression>DeepSeek-OCR: Contexts Optical Compression</a></li><li><a href=#see-the-text-from-tokenization-to-visual-reading>See The Text: From Tokenization to Visual Reading</a></li><li><a href=#text-or-pixels-it-takes-half-on-the-token-efficiency-of-visual-text-inputs>Text or Pixels? It Takes Half: On The Token Efficiency Of Visual Text Inputs</a></li><li><a href=#bonus-andrej-karpathy-tweet>bonus: Andrej Karpathy tweet</a></li></ul></li><li><a href=#main-arguments>Main Arguments</a><ul><li><ul><li><a href=#pixel-based-representations-of-text-are-worse>pixel-based representations of text are worse</a></li><li><a href=#over-eagerness-to-make-analogies-to-biological-brains>Over-eagerness to make analogies to biological brains</a></li><li><a href=#vision-isnt-even-how-humans-understand-language-fundamentally>vision isn&rsquo;t even how humans understand language (fundamentally)</a></li><li><a href=#why-the-comparison-between-visual-and-text-tokens-isnt-fair>Why the comparison between visual and text tokens isn&rsquo;t fair</a></li><li><a href=#why-we-should-consider-adding-the-text-encoder-back-in-some-form>Why we should consider adding the text encoder back (in some form)</a></li></ul></li></ul></li><li><a href=#conclusion>Conclusion</a></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><p>TLDR: if you have a text input and want to compress the number of tokens fed to your decoder, use a text-based encoder, don&rsquo;t render an image and use a visual encoder.</p><h2 class="relative group">Introduction and Context<div id=introduction-and-context class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#introduction-and-context aria-label=Anchor>#</a></span></h2><p>Today I was made aware of a recent line of research in Multimodal LLMs that attempts to reduce the number of tokens it takes
to represent text. The general approach is to take text, render it as a set of images, then pass it to a multi-modal LLM. The image
is parsed by a visual encoder that transforms those images into visual tokens that are then read by the LLM. The advantage of doing this,
it is argued, is that the number of visual tokens used to represent the images of the text is less than the number of tokens used to represent
the text directly, if you were to use the predominant sub-word tokenizers.</p><p>The summary of my argument is that I think rather than transforming text into an image and using a visual encoder to get compressed visual tokens in order to reduce the end number of tokens fed into the decoder, I think it would be more fruitful to train a text-based encoder specifically for compressing text into compressed tokens. In addition, I don&rsquo;t believe the comparison between visual and textual tokens is &lsquo;fair&rsquo; and that it would be more beneficial to evaluate these compression techniques against other related works. Furthermore, I&rsquo;d also like to discuss the issue of people providing tenuous analogies to the human brain that in actuality aren&rsquo;t a rigorous explanation of how these models are working, arguing that they shouldn&rsquo;t be used to drive areas of research to the extent they are.</p><p>To be clear, I don&rsquo;t want to discount the advantages of a visual approach in understanding documents where layout, formatting and diagrams inform the meaning of the document. I&rsquo;m specifically concerned with the practice of taking existing Unicode text, and transforming it into images in order to reduce the number of tokens used to represent that text.</p><p>I&rsquo;d like to acknowledge that the research I&rsquo;m talking about is done by people who know far more about Machine Learning than I do, however I still
feel that it&rsquo;s productive to write down my opinion, and in the worst case someone can help me understand why I&rsquo;m wrong. I&rsquo;m only a student, but I&rsquo;d
still like to take a stab at offering my thoughts.</p><p>Before proceeding any further, I&rsquo;d like to give a quick primer on LLMs so that those with a less technical background can understand for the purposes
of this article what a &lsquo;sub-word tokenizer&rsquo; and &lsquo;visual encoder&rsquo; are. Some points in this post will remain technical, but hopefully this should help frame the problem.</p><h2 class="relative group">High level overview of how it works<div id=high-level-overview-of-how-it-works class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#high-level-overview-of-how-it-works aria-label=Anchor>#</a></span></h2><h3 class="relative group">How LLMs see text and images<div id=how-llms-see-text-and-images class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#how-llms-see-text-and-images aria-label=Anchor>#</a></span></h3><p>You might have heard that LLMs predict the next &rsquo;token&rsquo;. A &rsquo;token&rsquo; is the equivalent of a character in language, it&rsquo;s the smallest element in a language. However, LLMs when looking
at text don&rsquo;t see individual characters, instead they normally &lsquo;see&rsquo; words or subwords (which just means a part of a word)<figure><img class="my-0 rounded-md" loading=lazy srcset="/notebook/on_visual_text_context_compression/tokenization_example_hu06993b76bf9dcc7aebcaa73e3328a0be_32029_330x0_resize_box_3.png 330w,
/notebook/on_visual_text_context_compression/tokenization_example_hu06993b76bf9dcc7aebcaa73e3328a0be_32029_660x0_resize_box_3.png 660w,
/notebook/on_visual_text_context_compression/tokenization_example_hu06993b76bf9dcc7aebcaa73e3328a0be_32029_1024x0_resize_box_3.png 1024w,
/notebook/on_visual_text_context_compression/tokenization_example_hu06993b76bf9dcc7aebcaa73e3328a0be_32029_1320x0_resize_box_3.png 2x" src=/notebook/on_visual_text_context_compression/tokenization_example_hu06993b76bf9dcc7aebcaa73e3328a0be_32029_660x0_resize_box_3.png alt="tokenization example"></figure></p><p>Each of these text tokens is mapped to an embedding, which is a vector that represents that word in a way that the machine can understand.</p><p>When we want to make LLMs multi-modal, we need a way of taking things like images and putting it into this format that LLMs understand, a list of tokens (vectors). The traditional way to do that is to have a &lsquo;visual encoder&rsquo;, which takes as input an image and as output produces a list of tokens (in vector form, the machine language) that represent the image. You can imagine the LLM as a blind man, and the &lsquo;visual encoder&rsquo; as a helper, who can describe to him (in machine language) what is happening in the image.</p><p>Researchers have found that the number of tokens produced by a visual encoder when looking at an image of text is often less than the number of tokens used if we had gone through each (sub)word and converted each one into a vector. Under the analogy, if the helper was to look at a page in a book and describe the page to the man, this description would be more concise than if the old man were to read it himself (using braille). Part of the reason why is that the helper is doing some sort of mild summarisation of the text (e.g. changing a long wordy sentence to a more quick, to-the-point one), as well as he is speaking in machine language, which is potentially more concise than long winded English. The argument then, by the researchers, is why not stop the blind man reading by himself and instead use the helper to help the blind man read faster or more?</p><h3 class="relative group">What is compression, and how does it apply here?<div id=what-is-compression-and-how-does-it-apply-here class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#what-is-compression-and-how-does-it-apply-here aria-label=Anchor>#</a></span></h3><p>When you take some information, and describe it using a smaller amount of information, we call that &lsquo;compression&rsquo;. A form of compression that people will often use is a &lsquo;zip&rsquo; folder. When I say that machine language is more concise than English in the analogy, what I mean in technical terms is that you might be turning each (sub)word into a vector of let&rsquo;s say 100 numbers, which is the size of each token. However, there might be cases where you don&rsquo;t need all 100 numbers to convey the meaning of that subword, meaning some of the numbers are &lsquo;wasted&rsquo;, or you could use the 100 numbers to express multiple subwords. For example, imagine you have the sentence &ldquo;mmmm, ok, alright&rdquo;. If you just go through subword by subword, then you get 100 numbers for &lsquo;mm&rsquo;, then another 100 for the next &lsquo;mm&rsquo;, then 100 for &lsquo;,&rsquo; etc. However, the visual encoder might look at that sentence and think actually, it can summarise the whole meaning in a single token of 100 numbers. In this way it can potentially convey the exact same sentence, but using fewer numbers (and thus fewer tokens).</p><p>The researchers thus compress the text by taking the text, and then turning it into a series of images, passing it through the visual encoder and then passing these visual tokens to the main LLM (which I will sometimes refer to as the decoder). My argument is that instead of taking text, then turning it into an image, then turning that back into some compressed text, why not just directly get the input text and compress it.</p><h2 class="relative group">Comments on recent papers<div id=comments-on-recent-papers class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#comments-on-recent-papers aria-label=Anchor>#</a></span></h2><p>This isn&rsquo;t a thorough analysis, I&rsquo;m just going to go through snippets of recent papers on the topic that I have comments on.</p><h3 class="relative group">DeepSeek-OCR: Contexts Optical Compression<div id=deepseek-ocr-contexts-optical-compression class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#deepseek-ocr-contexts-optical-compression aria-label=Anchor>#</a></span></h3><p><a href=https://arxiv.org/pdf/2510.18234v1 target=_blank>Read the paper</a></p><blockquote><p>a crucial research question that current models have not addressed is: for a document containing 1000 words, how many vision tokens are at least needed for decoding?</p></blockquote><p>I think that this is an interesting question, however the question I posit, which I think is more useful, is &ldquo;for a document containing 1000 words, how many (compressed) text tokens are at least needed for decoding?&rdquo;.</p><blockquote><p>this approach does not bring any overhead because it can leverage VLM infrastructure, as multimodal systems inherently require an additional vision encoder.</p></blockquote><p>I somewhat disagree to the extent that in order to use visual representations of text instead, then the encoder needs to be trained to take on this larger burden. It may be that a provider would want the visual encoder to be able to represent and compress text perfectly anyway, however their sophisticated architecture that involves being able to dynamically change or control the amount of compression done on the image definitely seems like an extra step.</p><p>Overall, though, I think this is a cool paper.</p><h3 class="relative group">See The Text: From Tokenization to Visual Reading<div id=see-the-text-from-tokenization-to-visual-reading class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#see-the-text-from-tokenization-to-visual-reading aria-label=Anchor>#</a></span></h3><p><a href=https://arxiv.org/pdf/2510.18840 target=_blank>Read the paper</a></p><blockquote><p>Humans read by recognizing words as visual objects, including their shapes, layouts, and patterns, before connecting them to meaning, which enables us to handle typos, distorted fonts, and various scripts effectively.</p></blockquote><p>While this is true to an extent, when I&rsquo;m writing a prompt to my LLM, all of the meaning needed is in those words, I don&rsquo;t need it to be considering the shape of each letter. There are definitely fine details of how humans look at words, but for me, when I&rsquo;m reading, what I&rsquo;m doing for the most part, or so it feels like, is just connecting words to their meaning. Sure, a visual medium helps handle typos, but I don&rsquo;t feel like this is a major weakness of existing LLMs that prevents them from functioning well, and I&rsquo;d argue that an LLM with character-level tokenization should be just as capable of identifying typos as one trained on visual images.</p><blockquote><p>Modern large language models (LLMs), however, rely on subword tokenization, fragmenting text into pieces from a fixed vocabulary. While effective for high-resource languages, this approach over-segments low-resource languages, yielding long, linguistically meaningless sequences and inflating computation</p></blockquote><p>The idea of considering the &lsquo;information-density&rsquo; of the specific language or content you&rsquo;re considering when tokenizing or compressing seems smart, and I don&rsquo;t disagree that you can have long sequences of text that don&rsquo;t mean much, but I don&rsquo;t see how changing that to an even longer list of meaningless pixels helps.</p><p><figure><img class="my-0 rounded-md" loading=lazy srcset="/notebook/on_visual_text_context_compression/diagram_of_brain_hu04401042d5dae34e6df1e79eacbf30af_97005_330x0_resize_box_3.png 330w,
/notebook/on_visual_text_context_compression/diagram_of_brain_hu04401042d5dae34e6df1e79eacbf30af_97005_660x0_resize_box_3.png 660w,
/notebook/on_visual_text_context_compression/diagram_of_brain_hu04401042d5dae34e6df1e79eacbf30af_97005_1024x0_resize_box_3.png 1024w,
/notebook/on_visual_text_context_compression/diagram_of_brain_hu04401042d5dae34e6df1e79eacbf30af_97005_1320x0_resize_box_3.png 2x" src=/notebook/on_visual_text_context_compression/diagram_of_brain_hu04401042d5dae34e6df1e79eacbf30af_97005_660x0_resize_box_3.png alt=diagram_of_brain><figcaption>Figure 1. from the paper</figcaption></figure></p><p>This reminds me of the Hierarchical Reasoning Model paper using mouse brains as inspiration/justification for the architecture. The argument is that the brain follows a pipeline of:</p><ol><li>taking a visual input</li><li>converting that input into a word</li><li>converting that word into meaning</li></ol><p>Personally, I&rsquo;m not against taking inspiration from nature, but I think the comparison to the human brain is a stretch. It also seems strange to me that if we already have the input as a sequence of words (step 2), we would choose to obfuscate the text (turning it into an image) just to follow the full pipeline that humans go through, rather than continuing from the step we&rsquo;re already on.</p><h3 class="relative group">Text or Pixels? It Takes Half: On The Token Efficiency Of Visual Text Inputs<div id=text-or-pixels-it-takes-half-on-the-token-efficiency-of-visual-text-inputs class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#text-or-pixels-it-takes-half-on-the-token-efficiency-of-visual-text-inputs aria-label=Anchor>#</a></span></h3><blockquote><p>Although vision encoding adds some overhead on smaller models, the shorter decoder sequence yields up to 45% end-to-end speedup on larger ones, demonstrating that off-the-shelf multimodal LLMs can treat images as an implicit compression layer, preserving performance at nearly half the original text-token cost.</p></blockquote><p>My beliefs align more with those presented in this paper. They recognise it&rsquo;s an off-the-shelf solution that uses existing tools to achieve the goal of compression implicitly, rather than making any sort of special proclamations about the properties of human vision to justify the use of a visual representation outside of allowing us to use existing encoders integrated with LLMs.</p><h3 class="relative group">bonus: Andrej Karpathy tweet<div id=bonus-andrej-karpathy-tweet class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#bonus-andrej-karpathy-tweet aria-label=Anchor>#</a></span></h3><p><a href="https://x.com/karpathy/status/1980397031542989305?t=BNvQ9xNArEoWBsVKtu-Ziw&amp;s=03" target=_blank>Original tweet</a></p><p>Andrej Karpathy also tweeted about the deepseek OCR paper:</p><blockquote><p>I quite like the new DeepSeek-OCR paper. It&rsquo;s a good OCR model (maybe a bit worse than dots), and yes data collection etc., but anyway it doesn&rsquo;t matter.</p><p>The more interesting part for me (esp as a computer vision at heart who is temporarily masquerading as a natural language person) is whether pixels are better inputs to LLMs than text. Whether text tokens are wasteful and just terrible, at the input.</p><p>Maybe it makes more sense that all inputs to LLMs should only ever be images. Even if you happen to have pure text input, maybe you&rsquo;d prefer to render it and then feed that in: - more information compression (see paper) => shorter context windows, more efficiency - significantly more general information stream => not just text, but e.g. bold text, colored text, arbitrary images. - input can now be processed with bidirectional attention easily and as default, not autoregressive attention - a lot more powerful. - delete the tokenizer (at the input)!! I already ranted about how much I dislike the tokenizer. Tokenizers are ugly, separate, not end-to-end stage. It &ldquo;imports&rdquo; all the ugliness of Unicode, byte encodings, it inherits a lot of historical baggage, security/jailbreak risk (e.g. continuation bytes). It makes two characters that look identical to the eye look as two completely different tokens internally in the network. A smiling emoji looks like a weird token, not an&mldr; actual smiling face, pixels and all, and all the transfer learning that brings along. The tokenizer must go.</p><p>OCR is just one of many useful vision -> text tasks. And text -> text tasks can be made to be vision ->text tasks. Not vice versa.</p><p>So many the User message is images, but the decoder (the Assistant response) remains text. It&rsquo;s a lot less obvious how to output pixels realistically&mldr; or if you&rsquo;d want to.</p><p>Now I have to also fight the urge to side quest an image-input-only version of nanochat&mldr;</p></blockquote><p>My quick thoughts:</p><ol><li>If you wanted to use bidirectional attention then why not train a bidirectional encoder on the text directly? The use of bidirectional attention and text representations are orthogonal.</li><li>I agree that a sub-word tokenizer is ugly, but I&rsquo;m not convinced rendering text as an image, turning it into patches, computing attention between those, and outputting some compressed visual tokens is any less ugly. You&rsquo;re essentially just swapping a linguistic tokenizer that deals in subwords for visual one that deals in patches.</li></ol><h2 class="relative group">Main Arguments<div id=main-arguments class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#main-arguments aria-label=Anchor>#</a></span></h2><h4 class="relative group">pixel-based representations of text are worse<div id=pixel-based-representations-of-text-are-worse class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#pixel-based-representations-of-text-are-worse aria-label=Anchor>#</a></span></h4><p>Turning text into an image fundamentally goes against common machine learning sense; you are going from a &lsquo;decent&rsquo; abstract representation (e.g. index of a word or half of a word) to a bad one with lots of redundancy. The font or position of text in an image is not relevant to the meaning of it, so it doesn&rsquo;t make sense to add &rsquo;noise&rsquo; to your input before compressing it.</p><p>Let&rsquo;s say you are doing a deepseek-esque experiment where you have a block of text, and you want to use some form of compression to make the minimum number of tokens (of a set dimensionality) such that the original text can be recovered from the input (and thus you can conclude that your method of compression doesn&rsquo;t lose too much of the original information). If you are first projecting the text to a visual representation and then feeding that to a visual encoder, and you get the original text at the end, then you can conclude either:</p><ol><li>the token representation contains the minimal amount of information needed to represent the text, in which case no visual information is conserved</li><li>the token representation also contains some amount of visual information</li></ol><p>In case 1, none of the visual information is being conserved, so there is no gain from transforming the text into an image. In case 2, the tokens contain some amount of &rsquo;extra&rsquo; information, meaning the input hasn&rsquo;t been compressed as much as possible.</p><h4 class="relative group">Over-eagerness to make analogies to biological brains<div id=over-eagerness-to-make-analogies-to-biological-brains class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#over-eagerness-to-make-analogies-to-biological-brains aria-label=Anchor>#</a></span></h4><p>Criticising anthropomorphism in AI in 2025 definitely feels like beating a dead horse, however I want to focus specifically on making analogies or drawing inspiration from animal brains when doing academic research. While this is an obvious and potentially good source of inspiration, it can often come across as somewhat hand-wavy, where the author selects a very specific feature of the way a brain works, and uses that to justify an architectural decision, despite the fact that this feature obviously operates in a very different context: the brain works very differently to an artificial neural network. The most recent example of this, which is why it&rsquo;s on my mind, is the Hierarchical Reasoning Model paper, which was quickly followed by the Tiny Recursive Model paper in response, which criticises the prior paper for not thoroughly justifying their architectural decisions and demonstrates how their explanation based on the brains of mice can be done away with in favour of a simpler explanation involving the use of a scratchpad.</p><p>Today in my lecture for Nature-Inspired Computing, we were discussing the paper <a href=https://onlinelibrary.wiley.com/doi/abs/10.1111/itor.12001 target=_blank>&ldquo;Metaheuristics - The Metaphor Exposed&rdquo;</a> in which the author criticises the over-use of metaphors and analogies to nature within the field of metaheuristics, saying that it makes it harder for a technical audience to understand the algorithms being employed.</p><blockquote><p>For a few decades, every year has seen the publication of several papers claiming to present a “novel” method for optimization, based on a metaphor of a process that is often seemingly completely unrelated to optimization. The jumps of frogs, the refraction of light, the flowing of water to the sea, an orchestra playing, sperm cells moving to fertilize an egg, the spiraling movements of galaxies, the colonizing behavior of empires, the behavior of bats, birds, ants, bees, flies, and virtually every other species of insects – it seems that there is not a single natural or man-made process that cannot be used as a metaphor for yet another “novel” optimization method</p></blockquote><p>Now I don&rsquo;t think that AI research suffers from this problem to the same extent as the field of metaheuristics as portrayed in this paper, in particular the issue he highlights where technical terms like &ldquo;population&rdquo; are renamed to &ldquo;harmony memory&rdquo; and &ldquo;sample&rdquo; is changed to &ldquo;note&rdquo;, however I do think both can potentially suffer from an over-reliance on nature to explain <em>why</em> an algorithm works. In metaheuristics, when you justify why an algorithm works using a line like &rsquo;this is analogous to how bats use echolocation to navigate a cave&rsquo; instead of &lsquo;it uses this specific way of doing local search which improves exploration&rsquo;, it makes the algorithm mysterious: it works because nature does it, and nature is right. Similarly, the brain can offer inspiration for ideas in AI, but when part of your justification for an architecture decision is &rsquo;this is analogous to how the brain does it&rsquo;, then it makes the algorithm harder to understand or build upon.</p><p>I believe that within the broader field of ML and AI, research on LLMs is particularly susceptible to this line of thinking. The argument presented in the &lsquo;See the Text&rsquo; paper that an LLM would somehow learn better if trained on images rather than text because that&rsquo;s how humans interpret the world seems to place LLMs on a pedestal of anthropomorphism that other Machine Learning models aren&rsquo;t given. For example, if you were trying to predict house prices using the California House Price dataset, would you first convert the number of rooms into an image of the number before feeding it to the model? Of course not. Just because LLMs seem more &ldquo;human-like&rdquo; doesn&rsquo;t necessarily mean that a good representation or architectural feature in a human brain would also be good for an artificial neural network.</p><h4 class="relative group">vision isn&rsquo;t even how humans understand language (fundamentally)<div id=vision-isnt-even-how-humans-understand-language-fundamentally class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#vision-isnt-even-how-humans-understand-language-fundamentally aria-label=Anchor>#</a></span></h4><p>In the case of emojis, we understand them visually in that the character&rsquo;s appearance is tied to its meaning, but this isn&rsquo;t generally the case for text; the shape of the letters in &lsquo;cat&rsquo; doesn&rsquo;t enhance or help my understanding of what the word means much. I could look at a word in Arabic and make simple guesses about the meaning based on the appearance, such as &ldquo;this word is really long, so it is less likely to be a very common word&rdquo;, but I couldn&rsquo;t get close to telling you the meaning of a sentence just by looking at the shape of it.</p><p>I should note that there are pictorial languages, for example Mandarin, but even these are very abstract and it is difficult to look at a character and say &lsquo;ah, that must mean cat&rsquo;.</p><p>Fundamentally, I think at least for me I understand language more like how an LLM views it: each word is (sort of) its own unique object in my brain, &lsquo;mat&rsquo; and &rsquo;nat&rsquo; are both just a combination of
letters that point to a different part in my brain. Of course, things like the layout of a document can change how you view text, but this research simply proposes taking a block of text as Unicode and then rendering
it as a block of text in an image.</p><p>Interestingly, as an aside, if you are interested in a visual representation then I think you could also make the case for a verbal representation of language, because how a language sounds has evolved over a long period of time to convey some sort of meaning. Again, you can&rsquo;t understand what another person is saying just from the sound of the word, but at least to English speakers there are definitely patterns that inform how we understand words, like &ldquo;lots of consonants makes a word sound harsher&rdquo;.</p><h4 class="relative group">Why the comparison between visual and text tokens isn&rsquo;t fair<div id=why-the-comparison-between-visual-and-text-tokens-isnt-fair class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-the-comparison-between-visual-and-text-tokens-isnt-fair aria-label=Anchor>#</a></span></h4><p>The &lsquo;See the Text&rsquo; paper says:</p><blockquote><p>SEETOK first renders text into images and leverages the visual encoders of pretrained MLLMs (e.g., Qwen2.5-VL) to extract textual representations, which are then passed to the LLM backbone for deeper processing.</p></blockquote><p>I believe that in some cases when people are discussing the compression of text using these methods they directly compare visual tokens and text tokens as if both are just &ldquo;textual representations&rdquo;, or two different ways of representing the same thing (e.g. a single word in isolation). In actuality, they are very different. In the case of the visual tokens they have already been pre-processed, including using local and global attention with reference to other parts of the image, so the tokens are both more compressed and conceptually &lsquo;richer&rsquo;. Therefore, in some sense it isn&rsquo;t &lsquo;fair&rsquo; to measure how many visual tokens it takes with this preprocessing stage to the number of text tokens it takes without any such processing. Of course it is useful to look at the amount of compression achieved, but not one of the papers provides a comparison of their method against other methods to compress the context.</p><h4 class="relative group">Why we should consider adding the text encoder back (in some form)<div id=why-we-should-consider-adding-the-text-encoder-back-in-some-form class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-we-should-consider-adding-the-text-encoder-back-in-some-form aria-label=Anchor>#</a></span></h4><p>Originally the transformer architecture used both an encoder and a decoder, however LLM architectures subsequently moved to becoming decoder only due to the significant speed-up in training when just using masked self-attention. However, I believe that this work on visual encoders as compression tools highlights the possibility of their return, in some form. In the original conception of the transformer, the architecture used a similarly sized encoder as decoder which in turn meant that the slowdown from training the encoder was very significant. However, the work highlighted here shows that you can take a relatively small encoder and large decoder and use the encoder to do some compression of the input, which then gives large computational savings due to the reduced number of tokens in the context of the decoder.</p><p>Therefore, while the encoder might originally have been a barrier to efficient training, it could potentially be used in the future as an opportunity to increase the efficiency of the decoder, where the majority of computational resources are now spent.</p><h2 class="relative group">Conclusion<div id=conclusion class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#conclusion aria-label=Anchor>#</a></span></h2><p>Using a visual encoder to compress text, while a novel idea, ultimately feels to me more of a clever workaround that utilises existing tools than an elegant long term solution. The root problem isn&rsquo;t that text is an unsuitable input, it&rsquo;s that current subword tokenizers are inefficient. The best way to address the problem is to directly attend to the text-processing pipeline itself. To put it simply, I think the best way to compress text is to use an encoder trained on compressing text. This is why I think that the return of a small dedicated text encoder, designed specifically to compress text tokens for a large decoder, is a more promising avenue for future research.</p><p>Right now I don&rsquo;t have the time or the resources to go about training an encoder (and a decoder trained to use it), so I shall leave this rant here and hopefully in a few months someone will put out a paper doing this in a way that achieves better compression than the visual encoding approach and I can say &ldquo;I told you so&rdquo;.</p></div></div><script>var oid="views_notebook/on_visual_text_context_compression/index.md",oid_likes="likes_notebook/on_visual_text_context_compression/index.md"</script><script type=text/javascript src=/js/page.min.b06a29d42a4ed16787978e2eee1e8c797b7698db2bc14ccee78f5c80ac566fc996190a73ad80a5e987558474b20b96fa38f7d85b405f165ff72b7b163c5ad11b.js integrity="sha512-sGop1CpO0WeHl44u7h6MeXt2mNsrwUzO549cgKxWb8mWGQpzrYCl6YdVhHSyC5b6OPfYW0BfFl/3K3sWPFrRGw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/notebook/genai_being_spooky/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Generative AI Being Spooky and Strange</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-05-23T00:00:00+00:00>23 May 2025</time>
</span></span></a></span><span></span></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025
Barnaby Wreford</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/js/process.min.62060bb247f4de2b6dde45903668fefb68d792f365587605177b1227c0cf43588701edaca0cb40e2c8e2789bd5ce67c1d2a215b9fb258c3496a7cd25e7cb5fdf.js integrity="sha512-YgYLskf03itt3kWQNmj++2jXkvNlWHYFF3sSJ8DPQ1iHAe2soMtA4sjieJvVzmfB0qIVufsljDSWp80l58tf3w=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://barnabywreford.co.uk style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body></html>