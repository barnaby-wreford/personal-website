<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=dark data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><title>High Dimension Vector Orthogonality and Superposition in LLMs &#183; Barnaby's Blog</title>
<meta name=title content="High Dimension Vector Orthogonality and Superposition in LLMs &#183; Barnaby's Blog"><meta name=description content="A statistical explanation to give intuition behind why vectors in high dimensions become increasingly more likely to be orthogonal, and why this is relevant for neural networks"><meta name=keywords content="Maths,AI,"><link rel=canonical href=https://barnabywreford.co.uk/problems/vector_orthogonality_and_llm_superposition/><link type=text/css rel=stylesheet href=/css/main.bundle.min.4ed2220b06454610a8a494fc6a9246b704e91bf845506e882a226f03c88b69098edc150c8f38844da43e356b1c3891161cd1b0ca131847ee658578ce4983ac59.css integrity="sha512-TtIiCwZFRhCopJT8apJGtwTpG/hFUG6IKiJvA8iLaQmO3BUMjziETaQ+NWscOJEWHNGwyhMYR+5lhXjOSYOsWQ=="><script type=text/javascript src=/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/js/main.bundle.min.3fc65be8df47ebc01260b43d7620a42d41769de3b3933d9263b4ec813e87828f9a851f1127ffff8ec45a65cd326726cf1f1ac529f341039c65f3f887e8fd6051.js integrity="sha512-P8Zb6N9H68ASYLQ9diCkLUF2neOzkz2SY7TsgT6Hgo+ahR8RJ///jsRaZc0yZybPHxrFKfNBA5xl8/iH6P1gUQ==" data-copy data-copied></script><script src=/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><meta property="og:title" content="High Dimension Vector Orthogonality and Superposition in LLMs"><meta property="og:description" content="A statistical explanation to give intuition behind why vectors in high dimensions become increasingly more likely to be orthogonal, and why this is relevant for neural networks"><meta property="og:type" content="article"><meta property="og:url" content="https://barnabywreford.co.uk/problems/vector_orthogonality_and_llm_superposition/"><meta property="og:image" content="https://barnabywreford.co.uk/problems/vector_orthogonality_and_llm_superposition/similar_feature_limitation_1.png"><meta property="article:section" content="problems"><meta property="article:published_time" content="2025-05-16T00:00:00+00:00"><meta property="article:modified_time" content="2025-05-16T00:00:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://barnabywreford.co.uk/problems/vector_orthogonality_and_llm_superposition/similar_feature_limitation_1.png"><meta name=twitter:title content="High Dimension Vector Orthogonality and Superposition in LLMs"><meta name=twitter:description content="A statistical explanation to give intuition behind why vectors in high dimensions become increasingly more likely to be orthogonal, and why this is relevant for neural networks"><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Problems","name":"High Dimension Vector Orthogonality and Superposition in LLMs","headline":"High Dimension Vector Orthogonality and Superposition in LLMs","description":"A statistical explanation to give intuition behind why vectors in high dimensions become increasingly more likely to be orthogonal, and why this is relevant for neural networks","abstract":"If you\u0026rsquo;re interested in experimenting yourself with the topics in this post, you may want to check out the code I used to generate all the graphs and stats in the accompanying jupyter notebook.","inLanguage":"en","url":"https:\/\/barnabywreford.co.uk\/problems\/vector_orthogonality_and_llm_superposition\/","author":{"@type":"Person","name":"Barnaby Wreford"},"copyrightYear":"2025","dateCreated":"2025-05-16T00:00:00\u002b00:00","datePublished":"2025-05-16T00:00:00\u002b00:00","dateModified":"2025-05-16T00:00:00\u002b00:00","keywords":["Maths","AI"],"mainEntityOfPage":"true","wordCount":"2211"}]</script><meta name=author content="Barnaby Wreford"><link href=mailto:bwreford.contact@gmail.com rel=me><link href=https://github.com/barnaby-wreford rel=me><link href=https://www.linkedin.com/in/barnaby-wreford-8a9241318 rel=me><script src=/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><link type=text/css rel=stylesheet href=/lib/katex/katex.min.e9afe31fe3937d5e7a56cf4c9be2a5ed9b1758944cdbe115f890eb5bb1cb1da7f8394dbef5e2e41ca15bce94926df2796fad26fbbfed4007afaf3bbb34fed0da.css integrity="sha512-6a/jH+OTfV56Vs9Mm+Kl7ZsXWJRM2+EV+JDrW7HLHaf4OU2+9eLkHKFbzpSSbfJ5b60m+7/tQAevrzu7NP7Q2g=="><script defer src=/lib/katex/katex.min.1649a56c18808fc7e07e867ccc9f9b650094e69754e798ab5e6273145fe65e54af06bdb1fe09b22c7691bb9892a5c0391f0e8230b0821528c1d469a0b4d2f642.js integrity="sha512-FkmlbBiAj8fgfoZ8zJ+bZQCU5pdU55irXmJzFF/mXlSvBr2x/gmyLHaRu5iSpcA5Hw6CMLCCFSjB1GmgtNL2Qg=="></script><script defer src=/lib/katex/auto-render.min.e9b2833d28623d18c071d78ef13e9c79d695122d296af3dbcee7bf1bf6518b0565bab59939267fbc8f5faf696193c20f5caef3e7501969cfb306f6738032730d.js integrity="sha512-6bKDPShiPRjAcdeO8T6cedaVEi0pavPbzue/G/ZRiwVlurWZOSZ/vI9fr2lhk8IPXK7z51AZac+zBvZzgDJzDQ==" onload=renderMathInElement(document.body)></script><script defer src=/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-YVTB4ZDJ50"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-YVTB4ZDJ50")</script><meta name=theme-color></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start gap-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ class="text-base font-medium text-gray-500 hover:text-gray-900">Barnaby&rsquo;s Blog</a></nav><nav class="hidden md:flex items-center gap-x-5 md:ml-12 h-12"><a href=/problems/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=Problems>Problems</p></a><a href=/projects/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=Projects>Projects</p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center gap-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400 ltr:mr-1 rtl:ml-1"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/problems/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=Problems>Problems</p></a></li><li class=mt-1><a href=/projects/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=Projects>Projects</p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><figure><img class="w-full rounded-lg single_hero_round nozoom" alt="High Dimension Vector Orthogonality and Superposition in LLMs" width=1200 height=1186 src=/problems/vector_orthogonality_and_llm_superposition/similar_feature_limitation_1_hu895f84dcad5ba3e2ba33600c68a16c4a_58519_1200x0_resize_box_3.png></figure><header id=single_header class="mt-5 max-w-prose"><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">High Dimension Vector Orthogonality and Superposition in LLMs</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2025-05-16T00:00:00+00:00>16 May 2025</time><span class="px-2 text-primary-500">&#183;</span><span>2211 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">11 mins</span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="Barnaby Wreford" src=/profile_hu68471ce5249335158e00a42614510882_266732_192x192_fill_box_center_3.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">Barnaby Wreford</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=mailto:bwreford.contact@gmail.com target=_blank aria-label=Email rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/barnaby-wreford target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://www.linkedin.com/in/barnaby-wreford-8a9241318 target=_blank aria-label=Linkedin rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a></li><li><a href=#intuition-using-dice>Intuition Using Dice</a></li><li><a href=#angle-between-vectors>Angle Between Vectors</a><ul><li><a href=#pairs>Pairs</a></li><li><a href=#larger-groups>Larger Groups</a></li></ul></li><li><a href=#application-in-neural-networks>Application in Neural Networks</a><ul><li><a href=#a-sense-of-scale-using-llama-3>A Sense of Scale Using LLaMA 3</a></li><li><a href=#linking-feature-sparsity-and-superposition>Linking Feature Sparsity and Superposition</a></li></ul></li><li><a href=#conclusion-and-further-reading>Conclusion and Further Reading</a></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a></li><li><a href=#intuition-using-dice>Intuition Using Dice</a></li><li><a href=#angle-between-vectors>Angle Between Vectors</a><ul><li><a href=#pairs>Pairs</a></li><li><a href=#larger-groups>Larger Groups</a></li></ul></li><li><a href=#application-in-neural-networks>Application in Neural Networks</a><ul><li><a href=#a-sense-of-scale-using-llama-3>A Sense of Scale Using LLaMA 3</a></li><li><a href=#linking-feature-sparsity-and-superposition>Linking Feature Sparsity and Superposition</a></li></ul></li><li><a href=#conclusion-and-further-reading>Conclusion and Further Reading</a></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><p>If you&rsquo;re interested in experimenting yourself with the topics in this post, you may want to check out the code I used to generate all the graphs and stats in the accompanying <a href=https://github.com/barnaby-wreford/vector_orthogonality_article/blob/main/explanation.ipynb target=_blank>jupyter notebook</a>.</p><h2 class="relative group">Introduction<div id=introduction class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#introduction aria-label=Anchor>#</a></span></h2><p>Reasoning about high dimensional spaces using geometric intuition is hard, our brains just aren&rsquo;t build to imagine such spaces. My aim in this article is to give a stastical intuition as to why the angle between two random vectors becomes increasingly likely to be close to 90 degrees, in other words the vectors become more orthogonal, as you increase the number of dimensions of those vectors.</p><p>In my opinion, the sparsity in terms of angles is a little less intuitive than sparsity in terms of distance between points: I can imagine that as you go from a line, to a square, to a cube, the amount of space (e.g. volume) contained within the object is increasing a lot, so it makes sense that points in high dimensional spaces tend to be further apart.</p><figure><img class="my-0 rounded-md" srcset="/problems/vector_orthogonality_and_llm_superposition/line_square_cube_packing_hu6bd30b100dcb862fc0be24b2cde88c5f_50407_330x0_resize_box_3.png 330w,
/problems/vector_orthogonality_and_llm_superposition/line_square_cube_packing_hu6bd30b100dcb862fc0be24b2cde88c5f_50407_660x0_resize_box_3.png 660w,
/problems/vector_orthogonality_and_llm_superposition/line_square_cube_packing_hu6bd30b100dcb862fc0be24b2cde88c5f_50407_1024x0_resize_box_3.png 1024w,
/problems/vector_orthogonality_and_llm_superposition/line_square_cube_packing_hu6bd30b100dcb862fc0be24b2cde88c5f_50407_1320x0_resize_box_3.png 2x" src=/problems/vector_orthogonality_and_llm_superposition/line_square_cube_packing_hu6bd30b100dcb862fc0be24b2cde88c5f_50407_660x0_resize_box_3.png alt="Comparison of the space covered by a centre point of radius 0.5, in a unit line, square and cube"><figcaption>Comparison of the space covered by a centre point of radius 0.5, in the unit space up to 3D. The proportion of the space that is close to this centre point decreases as dimensions increase.</figcaption></figure><p>However, the sparsity in terms of angles is less obvious, at least to me. When I imagine putting two random points in a square, and then a cube, it doesn&rsquo;t feel like the distribution of the angle between the points (and the origin) is trending in any obvious direction. This is where viewing the problem statistically can be useful.</p><h2 class="relative group">Intuition Using Dice<div id=intuition-using-dice class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#intuition-using-dice aria-label=Anchor>#</a></span></h2><p>Mathematically, we express the angle between two vectors using the equation:</p><p>$$
\cos{\theta} = \frac{\mathbf{a} \cdot \mathbf{b}}{|\mathbf{a}| |\mathbf{b}|}
$$</p><p>When the two vectors are nearly orthagonal, this means \(\cos{\theta}\) is nearly 0. If we assume that as we vary our vectors over different dimensions we keep the magnitude of those vectors the same for simplicity, this must mean we are looking to our dot product \(a \cdot b\) to achieve this near 0 behaviour. If we were to choose our vector directions randomly, then this dot product is a summation of random numbers.</p><p>To more intuitively imagine a summation of random numbers, imagine rolling a dice with values \([-3,-2,-1,1,2,3]\), rolling it many times and taking the sum. If you roll it twice, the minimum and maximum total values you can get is is \([-6,6]\), and you are reasonably likely to get close to either extreme. However, if you roll the dice 100 times, the min and max total values you can get is \([-300, 300]\), however you are likely to get a score (total value) much closer to 0 as most your rolls cancel each other out. The variance proportionally reducing as the number of trials increases is a common and fundamental theme in statistics.</p><p><figure><img class="my-0 rounded-md" loading=lazy srcset="/problems/vector_orthogonality_and_llm_superposition/dice_sums_n2_hu90273deb758f1a4e1fa2963fb3229b0b_34991_330x0_resize_box_3.png 330w,
/problems/vector_orthogonality_and_llm_superposition/dice_sums_n2_hu90273deb758f1a4e1fa2963fb3229b0b_34991_660x0_resize_box_3.png 660w,
/problems/vector_orthogonality_and_llm_superposition/dice_sums_n2_hu90273deb758f1a4e1fa2963fb3229b0b_34991_1024x0_resize_box_3.png 1024w,
/problems/vector_orthogonality_and_llm_superposition/dice_sums_n2_hu90273deb758f1a4e1fa2963fb3229b0b_34991_1320x0_resize_box_3.png 2x" src=/problems/vector_orthogonality_and_llm_superposition/dice_sums_n2_hu90273deb758f1a4e1fa2963fb3229b0b_34991_660x0_resize_box_3.png alt="Histogram of dice sums for two rolls"></figure></p><figure><img class="my-0 rounded-md" srcset="/problems/vector_orthogonality_and_llm_superposition/dice_sums_n100_huae940bf2ba480bfd9856e308a3494a31_44084_330x0_resize_box_3.png 330w,
/problems/vector_orthogonality_and_llm_superposition/dice_sums_n100_huae940bf2ba480bfd9856e308a3494a31_44084_660x0_resize_box_3.png 660w,
/problems/vector_orthogonality_and_llm_superposition/dice_sums_n100_huae940bf2ba480bfd9856e308a3494a31_44084_1024x0_resize_box_3.png 1024w,
/problems/vector_orthogonality_and_llm_superposition/dice_sums_n100_huae940bf2ba480bfd9856e308a3494a31_44084_1320x0_resize_box_3.png 2x" src=/problems/vector_orthogonality_and_llm_superposition/dice_sums_n100_huae940bf2ba480bfd9856e308a3494a31_44084_660x0_resize_box_3.png alt="Histogram of dice sums for 100 rolls"><figcaption>Note the change in scale along the x-axis (in all graphs)</figcaption></figure><p>In our original equation for the angle, we divide by the magnitude of the vectors. The simplest analogy in our dice example is to divide by the maximum value achievable using the dice. Therefore, the full &lsquo;game&rsquo; is to roll our dice \(n\) times, take the sum, then divide by \(3n\). I hope it is intuitive that as the number of rolls increases, more of our rolls as a proportion will be cancelled out by other rolls, and we are more likely to get a final weighted score that is closer to 0.</p><p><figure><img class="my-0 rounded-md" loading=lazy srcset="/problems/vector_orthogonality_and_llm_superposition/weighted_dice_sums_n2_hu891f337f253be43db8cb3625d964e812_35229_330x0_resize_box_3.png 330w,
/problems/vector_orthogonality_and_llm_superposition/weighted_dice_sums_n2_hu891f337f253be43db8cb3625d964e812_35229_660x0_resize_box_3.png 660w,
/problems/vector_orthogonality_and_llm_superposition/weighted_dice_sums_n2_hu891f337f253be43db8cb3625d964e812_35229_1024x0_resize_box_3.png 1024w,
/problems/vector_orthogonality_and_llm_superposition/weighted_dice_sums_n2_hu891f337f253be43db8cb3625d964e812_35229_1320x0_resize_box_3.png 2x" src=/problems/vector_orthogonality_and_llm_superposition/weighted_dice_sums_n2_hu891f337f253be43db8cb3625d964e812_35229_660x0_resize_box_3.png alt="Histogram of weighted dice sums for two rolls"></figure><figure><img class="my-0 rounded-md" loading=lazy srcset="/problems/vector_orthogonality_and_llm_superposition/weighted_dice_sums_n100_huae32bdb3198f7e302ef83dc91dbcf3a5_30928_330x0_resize_box_3.png 330w,
/problems/vector_orthogonality_and_llm_superposition/weighted_dice_sums_n100_huae32bdb3198f7e302ef83dc91dbcf3a5_30928_660x0_resize_box_3.png 660w,
/problems/vector_orthogonality_and_llm_superposition/weighted_dice_sums_n100_huae32bdb3198f7e302ef83dc91dbcf3a5_30928_1024x0_resize_box_3.png 1024w,
/problems/vector_orthogonality_and_llm_superposition/weighted_dice_sums_n100_huae32bdb3198f7e302ef83dc91dbcf3a5_30928_1320x0_resize_box_3.png 2x" src=/problems/vector_orthogonality_and_llm_superposition/weighted_dice_sums_n100_huae32bdb3198f7e302ef83dc91dbcf3a5_30928_660x0_resize_box_3.png alt="Histogram of weighted dice sums for 100 rolls"></figure></p><h2 class="relative group">Angle Between Vectors<div id=angle-between-vectors class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#angle-between-vectors aria-label=Anchor>#</a></span></h2><h3 class="relative group">Pairs<div id=pairs class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#pairs aria-label=Anchor>#</a></span></h3><p>Let&rsquo;s now move to viewing the distribution of vectors. As a reminder, what the dot product does is:</p><p>$$
\mathbf{a} \cdot \mathbf{b} = a_1 b_1 + a_2 b_2 + \cdots + a_n b_n
$$</p><p>And the equation for the angle is:
$$
\cos{\theta} = \frac{\mathbf{a} \cdot \mathbf{b}}{|\mathbf{a}| |\mathbf{b}|}
$$</p><p>To move from our dice game to the vector space, there are some subtle but not too impactful changes:</p><ul><li>we compute values in our sum by multiplying two random values from a and b together, instead of just rolling a singular die. (We are ultimately still just picking a random value, symmetrically distributed around 0)</li><li>we divide by the magnitude of the vector, instead of the magnitude of the best possible rolls (the big picture is both scale the output of the sum to between [-1,1])</li></ul><p>When we re-run the experiment using vectors, we unsurprisingly see similar behaviour as we scale up the number of dimensions</p><p><figure><img class="my-0 rounded-md" loading=lazy srcset="/problems/vector_orthogonality_and_llm_superposition/vector_angle_histogram_d3_huf12fcef31c76e994fc9f5bc071d161f0_41107_330x0_resize_box_3.png 330w,
/problems/vector_orthogonality_and_llm_superposition/vector_angle_histogram_d3_huf12fcef31c76e994fc9f5bc071d161f0_41107_660x0_resize_box_3.png 660w,
/problems/vector_orthogonality_and_llm_superposition/vector_angle_histogram_d3_huf12fcef31c76e994fc9f5bc071d161f0_41107_1024x0_resize_box_3.png 1024w,
/problems/vector_orthogonality_and_llm_superposition/vector_angle_histogram_d3_huf12fcef31c76e994fc9f5bc071d161f0_41107_1320x0_resize_box_3.png 2x" src=/problems/vector_orthogonality_and_llm_superposition/vector_angle_histogram_d3_huf12fcef31c76e994fc9f5bc071d161f0_41107_660x0_resize_box_3.png alt="Histogram of angle between two vectors in three dimensions"></figure><figure><img class="my-0 rounded-md" loading=lazy srcset="/problems/vector_orthogonality_and_llm_superposition/vector_angle_histogram_d100_hu184b9dd01de1f835789166e82055e47c_40437_330x0_resize_box_3.png 330w,
/problems/vector_orthogonality_and_llm_superposition/vector_angle_histogram_d100_hu184b9dd01de1f835789166e82055e47c_40437_660x0_resize_box_3.png 660w,
/problems/vector_orthogonality_and_llm_superposition/vector_angle_histogram_d100_hu184b9dd01de1f835789166e82055e47c_40437_1024x0_resize_box_3.png 1024w,
/problems/vector_orthogonality_and_llm_superposition/vector_angle_histogram_d100_hu184b9dd01de1f835789166e82055e47c_40437_1320x0_resize_box_3.png 2x" src=/problems/vector_orthogonality_and_llm_superposition/vector_angle_histogram_d100_hu184b9dd01de1f835789166e82055e47c_40437_660x0_resize_box_3.png alt="Histogram of angle between two vectors in 100 dimensions"></figure></p><h3 class="relative group">Larger Groups<div id=larger-groups class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#larger-groups aria-label=Anchor>#</a></span></h3><p>As pairs of vectors become increasingly more orthoganol as the number of dimensions increases, so do larger groups of vectors. Let&rsquo;s look at the minimum angle between any two vectors in a group, for different size groups in different dimensions.</p><p><figure><img class="my-0 rounded-md" loading=lazy srcset="/problems/vector_orthogonality_and_llm_superposition/group_vector_angle_histogram_d100_g2_hucd23badc57092e2eb8b90f0bd1a3dc22_42575_330x0_resize_box_3.png 330w,
/problems/vector_orthogonality_and_llm_superposition/group_vector_angle_histogram_d100_g2_hucd23badc57092e2eb8b90f0bd1a3dc22_42575_660x0_resize_box_3.png 660w,
/problems/vector_orthogonality_and_llm_superposition/group_vector_angle_histogram_d100_g2_hucd23badc57092e2eb8b90f0bd1a3dc22_42575_1024x0_resize_box_3.png 1024w,
/problems/vector_orthogonality_and_llm_superposition/group_vector_angle_histogram_d100_g2_hucd23badc57092e2eb8b90f0bd1a3dc22_42575_1320x0_resize_box_3.png 2x" src=/problems/vector_orthogonality_and_llm_superposition/group_vector_angle_histogram_d100_g2_hucd23badc57092e2eb8b90f0bd1a3dc22_42575_660x0_resize_box_3.png alt="Histogram of angle between two vectors in 100 dimensions"></figure><figure><img class="my-0 rounded-md" loading=lazy srcset="/problems/vector_orthogonality_and_llm_superposition/group_vector_angle_histogram_d100_g5_hu2e9203cf4ecffaac49a669c8526588e2_41877_330x0_resize_box_3.png 330w,
/problems/vector_orthogonality_and_llm_superposition/group_vector_angle_histogram_d100_g5_hu2e9203cf4ecffaac49a669c8526588e2_41877_660x0_resize_box_3.png 660w,
/problems/vector_orthogonality_and_llm_superposition/group_vector_angle_histogram_d100_g5_hu2e9203cf4ecffaac49a669c8526588e2_41877_1024x0_resize_box_3.png 1024w,
/problems/vector_orthogonality_and_llm_superposition/group_vector_angle_histogram_d100_g5_hu2e9203cf4ecffaac49a669c8526588e2_41877_1320x0_resize_box_3.png 2x" src=/problems/vector_orthogonality_and_llm_superposition/group_vector_angle_histogram_d100_g5_hu2e9203cf4ecffaac49a669c8526588e2_41877_660x0_resize_box_3.png alt="Histogram of angle between five vectors in 100 dimensions"></figure><figure><img class="my-0 rounded-md" loading=lazy srcset="/problems/vector_orthogonality_and_llm_superposition/group_vector_angle_histogram_d1000_g5_hu869edc7b229a2b150681dcee990af8ee_44031_330x0_resize_box_3.png 330w,
/problems/vector_orthogonality_and_llm_superposition/group_vector_angle_histogram_d1000_g5_hu869edc7b229a2b150681dcee990af8ee_44031_660x0_resize_box_3.png 660w,
/problems/vector_orthogonality_and_llm_superposition/group_vector_angle_histogram_d1000_g5_hu869edc7b229a2b150681dcee990af8ee_44031_1024x0_resize_box_3.png 1024w,
/problems/vector_orthogonality_and_llm_superposition/group_vector_angle_histogram_d1000_g5_hu869edc7b229a2b150681dcee990af8ee_44031_1320x0_resize_box_3.png 2x" src=/problems/vector_orthogonality_and_llm_superposition/group_vector_angle_histogram_d1000_g5_hu869edc7b229a2b150681dcee990af8ee_44031_660x0_resize_box_3.png alt="Histogram of angle between five vectors in 1000 dimensions"></figure></p><h2 class="relative group">Application in Neural Networks<div id=application-in-neural-networks class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#application-in-neural-networks aria-label=Anchor>#</a></span></h2><p>In modern LLMs, ideas (called features) are represented using vectors. For example, you might have a direction which represents size, so to represent the word &lsquo;huge&rsquo;, the LLM would assign it a vector that is strongly positive in the size direction. Superposition occurs when a single/similar direction in the model’s embedding space simultaneously represents multiple unrelated features, because the model has to pack many concepts into a limited number of dimensions. For example, if you look at the output of a neuron, it might have similar outputs when representing the idea of &lsquo;size&rsquo; and &lsquo;red&rsquo;, despite those two ideas being quite distinct.</p><figure><img class="my-0 rounded-md" srcset="/problems/vector_orthogonality_and_llm_superposition/close_vectors_hue0b31ab54133fcfbc4ed667ac1de6697_10202_330x0_resize_box_3.png 330w,
/problems/vector_orthogonality_and_llm_superposition/close_vectors_hue0b31ab54133fcfbc4ed667ac1de6697_10202_660x0_resize_box_3.png 660w,
/problems/vector_orthogonality_and_llm_superposition/close_vectors_hue0b31ab54133fcfbc4ed667ac1de6697_10202_1024x0_resize_box_3.png 1024w,
/problems/vector_orthogonality_and_llm_superposition/close_vectors_hue0b31ab54133fcfbc4ed667ac1de6697_10202_1320x0_resize_box_3.png 2x" src=/problems/vector_orthogonality_and_llm_superposition/close_vectors_hue0b31ab54133fcfbc4ed667ac1de6697_10202_660x0_resize_box_3.png alt="Histogram of dice sums for 100 rolls"><figcaption>It is unclear whether the point represents a lot of redness, size, or a combination</figcaption></figure><p>This is important in the field of mechanistic interpretability, which aims to try and reverse engineer neural networks into human understandable algorithms. In this field, superposition is a large barrier, because it makes it significantly harder to understand what a neuron activation means when it fires similarly for multiple different ideas (in this case, is the neuron firing because it is thinking about size, or redness?)</p><p>The goal of this section is to show why the number of near-orthagonal vectors in high dimensional spaces is often used as an explanation to why superposition appears in neural networks.</p><h3 class="relative group">A Sense of Scale Using LLaMA 3<div id=a-sense-of-scale-using-llama-3 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#a-sense-of-scale-using-llama-3 aria-label=Anchor>#</a></span></h3><p>The number of embedding dimensions, the number of dimensions used to encode a word, for modern LLMs tends to increase as the models overall size is scaled. For example, for LLaMA 3, the number of dimensions is:</p><table><thead><tr><th>Number of Parameters</th><th>Number of Dimensions (word embedding)</th></tr></thead><tbody><tr><td>8 Billion</td><td>4,096</td></tr><tr><td>70 Billion</td><td>8,192</td></tr><tr><td>405 Billion</td><td>16,384</td></tr></tbody></table><p>Let&rsquo;s take a look at the distribution of angles between any two random vectors in the embedding space of LLaMa 3&rsquo;s largest model at 405 Billion Parameters.</p><p><figure><img class="my-0 rounded-md" loading=lazy srcset="/problems/vector_orthogonality_and_llm_superposition/vector_angle_histogram_d16384_hu0f21bcb851c45d752238c5e6030c07d1_33036_330x0_resize_box_3.png 330w,
/problems/vector_orthogonality_and_llm_superposition/vector_angle_histogram_d16384_hu0f21bcb851c45d752238c5e6030c07d1_33036_660x0_resize_box_3.png 660w,
/problems/vector_orthogonality_and_llm_superposition/vector_angle_histogram_d16384_hu0f21bcb851c45d752238c5e6030c07d1_33036_1024x0_resize_box_3.png 1024w,
/problems/vector_orthogonality_and_llm_superposition/vector_angle_histogram_d16384_hu0f21bcb851c45d752238c5e6030c07d1_33036_1320x0_resize_box_3.png 2x" src=/problems/vector_orthogonality_and_llm_superposition/vector_angle_histogram_d16384_hu0f21bcb851c45d752238c5e6030c07d1_33036_660x0_resize_box_3.png alt="Histogram of angle between two vectors in 16384 dimensions"></figure></p><p>Remember these experiments are just to give an intuition about the sparsity of vectors by looking at random vectors. When LLMs are trained, they can essentially &lsquo;choose&rsquo; which vectors to use (associate with a given feature), meaning they can choose a packing which maximises orthagonality between many vectors. Alternatively, for a given required angle between all vectors, they can maximise the number of vectors they can use.</p><figure><img class="my-0 rounded-md" srcset="/problems/vector_orthogonality_and_llm_superposition/vector_pentagon_hud9320fd2df2e7a8829973a3891591116_13989_330x0_resize_box_3.png 330w,
/problems/vector_orthogonality_and_llm_superposition/vector_pentagon_hud9320fd2df2e7a8829973a3891591116_13989_660x0_resize_box_3.png 660w,
/problems/vector_orthogonality_and_llm_superposition/vector_pentagon_hud9320fd2df2e7a8829973a3891591116_13989_1024x0_resize_box_3.png 1024w,
/problems/vector_orthogonality_and_llm_superposition/vector_pentagon_hud9320fd2df2e7a8829973a3891591116_13989_1320x0_resize_box_3.png 2x" src=/problems/vector_orthogonality_and_llm_superposition/vector_pentagon_hud9320fd2df2e7a8829973a3891591116_13989_660x0_resize_box_3.png alt="5 vectors pointing from the origin to points on a pentagon"><figcaption>Max number of vectors in 2D with minimum angle separation of 72 degrees is 5</figcaption></figure><p>You have seen that the average fixed size group of vectors becomes more orthogonal to each other as the number of dimensions grows. In the same manner, the largest possible group you can make with a fixed required angle separation also grows as you increase the number of dimensions. For example, if you wanted to draw as many points on the edge of a circle as possible, where no two points are closer than 45 degrees, this would be less than the number of points you could draw on the surface of a sphere, where no two points are closer than 45 degrees.</p><div id=gallery-0f80a8a472af74ffe28641f64a7da70e class=gallery><img alt="dotted circle" src=circle_dots.png class=grid-w50>
<img alt="dotted sphere" src=sphere_dots.png class=grid-w50></div><p>In fact, the maximum size of a constrained group grows much faster than these graphs might indicate - it is known that the maximum number of near-orthogonal vectors scales exponentially with the number of dimensions.</p><p>The Kabatiansky-Levenshtein Bound gives an asymptotic formula for approximating this value (technically referred to as the maximal size of a spherical code in n dimensions). We can use this formula to illustrate the approximate shape and magnitude of the function relating the angle separation to the maximum group size for the dimensions used in LLaMA 3, however do not quote these values since the KL bound can become innaccurate for theta values close to 90 degrees.</p><table><thead><tr><th>Minimum Angle Separation (degrees)</th><th>Dimensions</th><th>Approx. Max Number of Vectors</th></tr></thead><tbody><tr><td>88</td><td>16,384</td><td>10^20</td></tr><tr><td>85</td><td>16,384</td><td>10^99</td></tr><tr><td>80</td><td>16,384</td><td>10^322</td></tr></tbody></table><h3 class="relative group">Linking Feature Sparsity and Superposition<div id=linking-feature-sparsity-and-superposition class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#linking-feature-sparsity-and-superposition aria-label=Anchor>#</a></span></h3><p>As can be observed from the group size graphs earlier, if you increase the group size the average minimum angle between vectors decreases. Therefore, if you want to limit the probability of getting a certain level of similarity between vectors, there is a maximum limit to the group size you can use. In a similar vein, a crucial aspect of the KL bound formula is that the less orthogonality you have between your vectors - in other words the more similar your vectors are - the greater number of vectors you can pack into your space.</p><p>In deep learning, you want to be able to distinguish different feature vectors, so there is a limit to the allowable similarity between these vectors. If one imagines this allowable similarity as a limited resource, then there are two forces competing for it: a desire to increase the &lsquo;group size&rsquo; (number of simultaneously active features) and a desire to increase the total number of vectors packed into the space (number of total representable features).</p><figure><img class="my-0 rounded-md" srcset="/problems/vector_orthogonality_and_llm_superposition/embedding_visualisation_comparison_huc1fd665bc8696aef4e6717797aa4adca_35692_330x0_resize_box_3.png 330w,
/problems/vector_orthogonality_and_llm_superposition/embedding_visualisation_comparison_huc1fd665bc8696aef4e6717797aa4adca_35692_660x0_resize_box_3.png 660w,
/problems/vector_orthogonality_and_llm_superposition/embedding_visualisation_comparison_huc1fd665bc8696aef4e6717797aa4adca_35692_1024x0_resize_box_3.png 1024w,
/problems/vector_orthogonality_and_llm_superposition/embedding_visualisation_comparison_huc1fd665bc8696aef4e6717797aa4adca_35692_1320x0_resize_box_3.png 2x" src=/problems/vector_orthogonality_and_llm_superposition/embedding_visualisation_comparison_huc1fd665bc8696aef4e6717797aa4adca_35692_660x0_resize_box_3.png alt="2 vs 5 feature visualisation"><figcaption>Comparison of representations with 2 vs 5 total representable features</figcaption></figure><p>To further understand why you might not be able to have as many simultaneously firing feature vectors, consider that the output of the neurons will be a point in the embedding space. In the first case, with just &lsquo;red&rsquo; and &lsquo;size&rsquo;, you can take any point and reverse engineer the vectors that were combined to make it. However, in the case where you have 5 total features and I show you a point that is a combination of two of them, you might struggle to confidently say for some points which vectors were &lsquo;combined&rsquo; to get that point:</p><div id=gallery-bdc63a8c681d0cd181e7edbdf052cd00 class=gallery><img alt="vector combination example 1" src=similar_feature_limitation_1.png class=grid-w50>
<img alt="vector combination example 2" src=similar_feature_limitation_2.png class=grid-w50>
<img alt="vector combination example 3" src=similar_feature_limitation_3.png class=grid-w50></div><p>Therefore, the model has to learn a trade-off in its representation. For example, consider how the two models above might represent a car, or a ladybird.</p><table><thead><tr><th>Total Known Features</th><th>Number of Active Features (group size)</th><th>Car Representation</th><th>Ladybird Representation</th></tr></thead><tbody><tr><td>Size, Red</td><td>2</td><td>Fairly big, not very red</td><td>small, very red</td></tr><tr><td>Size, Red, Vehicle, Beauty, Animal</td><td>1</td><td>Definitely a vehicle</td><td>very red?</td></tr></tbody></table><p>In reality, the model doesn&rsquo;t necessarily &ldquo;choose&rdquo; to only activate one feature, for example only representing a ladybird with &ldquo;very red&rdquo;. It will have detectors for &lsquo;redness&rsquo;, &lsquo;size&rsquo; and &lsquo;animal&rsquo;, and if it was given a ladybird it would signal strongly for all of them. Instead, the model has to learn features that are both useful and don&rsquo;t often appear all at once in the training dataset. For instance, if the training set contained many instances of ladybirds, the model learning 5 features may want to choose more specific and independent features. Alternatively, if there were far more instances of ladybirds than cars, another solution could be to remove the &lsquo;vehicle&rsquo; feature completely, to better focus on the remaining, more important features.</p><p>This is why the sparsity of features is a useful predictor for the amount of superposition in a neural network. &lsquo;Sparsity of features&rsquo; essentially means how often multiple features are active simultaneously as a proportion of the total identifiable features. Language can represent a vast array of ideas, but only a very small portion of those are present in a given word or sentence (or even LLM context window). This means the balance between the two fighting forces tips towards the total number of representable features, and as discussed before in order to be able to increase total features by packing more vectors into the space then you must accept more similar vectors for those features.</p><p>Greater similarity between feature vectors manifests as superposition, therefore language models like GPT pose a major challenge to interpretability researchers.</p><h2 class="relative group">Conclusion and Further Reading<div id=conclusion-and-further-reading class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#conclusion-and-further-reading aria-label=Anchor>#</a></span></h2><p>High dimensional spaces can seem intimidating, however often this stems from trying to force yourself to think about problems geometrically, when they might be easier understood through another lens. Having a good conceptual understanding of such spaces can help understand modern neural networks, since modern applications are centred around transformers that use high dimensionality embeddings. These models &rsquo;think&rsquo; in high dimensionality spaces, so to understand the models it would seem useful to also understand the characteristics of how their thoughts are represented.</p><p>Understanding how the angle between vectors can affect feature representations is just the tip of the iceberg when it comes to understanding neural networks. I hope to gain a further grasp of other fundamental concepts that constrain and affect neural networks, especially when applied to mechanistic interpretability, and I would encourage others too to explore the topic. If you&rsquo;re interested in mechanistic interpretability or AI safety more broadly, then Anthropic&rsquo;s research is a great place to start:</p><ul><li><a href=https://transformer-circuits.pub/2022/toy_model/index.html target=_blank>Toy Models of Superposition</a> - an in-depth exploration of superposition (a great follow up from this post)</li><li><a href=https://www.anthropic.com/news/alignment-faking target=_blank>Alignment Faking in Large Language Models</a> - short and non-technical example of how modern LLM&rsquo;s can strategically lie (the perfect introduction to AI safety)</li></ul></div></div><script>var oid="views_problems/vector_orthogonality_and_LLM_superposition/index.md",oid_likes="likes_problems/vector_orthogonality_and_LLM_superposition/index.md"</script><script type=text/javascript src=/js/page.min.b06a29d42a4ed16787978e2eee1e8c797b7698db2bc14ccee78f5c80ac566fc996190a73ad80a5e987558474b20b96fa38f7d85b405f165ff72b7b163c5ad11b.js integrity="sha512-sGop1CpO0WeHl44u7h6MeXt2mNsrwUzO549cgKxWb8mWGQpzrYCl6YdVhHSyC5b6OPfYW0BfFl/3K3sWPFrRGw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/problems/recurrence_relations_in_programming/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Pragmatic Recurrence Relations: Why the Closed Form Won't Help You</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-01-30T00:00:00+00:00>30 January 2025</time>
</span></span></a></span><span></span></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025
Barnaby Wreford</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/js/process.min.62060bb247f4de2b6dde45903668fefb68d792f365587605177b1227c0cf43588701edaca0cb40e2c8e2789bd5ce67c1d2a215b9fb258c3496a7cd25e7cb5fdf.js integrity="sha512-YgYLskf03itt3kWQNmj++2jXkvNlWHYFF3sSJ8DPQ1iHAe2soMtA4sjieJvVzmfB0qIVufsljDSWp80l58tf3w=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://barnabywreford.co.uk style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body></html>