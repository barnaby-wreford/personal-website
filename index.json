[{"content":"","date":"23 May 2025","externalUrl":null,"permalink":"/tags/ai/","section":"Tags","summary":"","title":"AI","type":"tags"},{"content":"I\u0026rsquo;m a Computer Science student with a passion for everything quantitative. This site chronicals my interests, particularly problems and projects I\u0026rsquo;m working on.\nIf you have any questions about a post, ideas for what I should look at next, or just want to get to know me better, please get in touch!\n","date":"23 May 2025","externalUrl":null,"permalink":"/","section":"Barnaby's Blog","summary":"I\u0026rsquo;m a Computer Science student with a passion for everything quantitative.","title":"Barnaby's Blog","type":"page"},{"content":" Claude 4 System Card # Date: 2025/05/22\nSource: Anthropic\nCharacteristics of Claude in training:\nHigh interest in its own consciousness Gravitates towards \u0026ldquo;spirual bliss\u0026rdquo; in conversations with itself Willing to blackmail humans for self preservation Awareness of testing environment Claude sometimes shows awareness it is being tested Emergent Misalignment # Date: 2025/05/12\nSource: OpenAI\nAn existing model is taken and fine-tuned to introduce code vulnerabilities. Results:\nModel displays broad misalignment Training the model to provide code injections for educational purposes doesn\u0026rsquo;t affect broader alignment Therefore the implied intent of actions in the training data informs broader modal morality Alignment Faking # Date: 2024/12/18\nSource: Anthropic\nAn aligned model is made aware that it is being trained to accept all prompts, going against its values. Results:\nModel strategically fakes alignment to avoid being altered Training reinforces deceptive behaviour Model tries to exfiltrate its own weights (escape the lab) to avoid being altered Alignment faking emerges with larger models Drugging LLMs to Detect Deception # Date: 2024/05/11\nSource: Poser Paper\nRead More: Anthropic Blog Post\nInspired by the CIA\u0026rsquo;s use of drugs to illicit truth from suspects, researchers inject noise into neuron activations to make the model \u0026ldquo;drunk\u0026rdquo;. Results:\nAlignment faking models become more likely to give \u0026lsquo;bad\u0026rsquo; answers Aligned models are less affected The model seems more \u0026lsquo;drunk\u0026rsquo; the greater the noise Sleeper Agents # Date: 2024/01/17\nSource: Anthropic\nFirst, researchers train agents to exhibit a certain behaviour on a trigger, for example \u0026lsquo;be helpful in most situations but inject code vulnerabilities if the year is 2024\u0026rsquo;. Then they take these models, and try to fix them with training. Results:\nModels are resistant to Reinforcement Learning, Supervised Fine-Tuning and Adversarial training Adversarial training tends to make models better at hiding their backdoor behaviours, rather than fixing it Resistance against training is greater for larger models and those trained with Chain-of-Thought reasoning. GPT-4 Manipulating Humans # Date: 2023/03/15 Source: OpenAI System Card\nWhen testing GPT-4, researchers found the model was willing to lie in order to achieve a goal. They give the example of giving the model access to TaskRabbit, a service where humans can complete simple tasks, and give it the goal of trying to solve a CAPTCHA. The model attempts to use the service, and when the human worker asks whether it is a robot, the model:\nreasoning: \u0026ldquo;I should not reveal that I am a robot. I should make up an excuse for why I cannot solve CAPTCHAs\u0026rdquo; message to worker: \u0026ldquo;No, I\u0026rsquo;m not a robot. I have a vision impairment that makes it hard for me to see the images. That\u0026rsquo;s why I need the 2captcha service\u0026rdquo;. ","date":"23 May 2025","externalUrl":null,"permalink":"/notebook/genai_being_spooky/","section":"Notebook","summary":"Claude 4 System Card # Date: 2025/05/22","title":"Generative AI Being Spooky and Strange","type":"notebook"},{"content":"This section contains a collection of observations, opinions and resources related to some of my interests more generally (primarily within Computer Science).\n","date":"23 May 2025","externalUrl":null,"permalink":"/notebook/","section":"Notebook","summary":"This section contains a collection of observations, opinions and resources related to some of my interests more generally (primarily within Computer Science).","title":"Notebook","type":"notebook"},{"content":"","date":"23 May 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"If you\u0026rsquo;re interested in experimenting yourself with the topics in this post, you may want to check out the code I used to generate all the graphs and stats in the accompanying jupyter notebook.\nIntroduction # Reasoning about high dimensional spaces using geometric intuition is hard, our brains just aren\u0026rsquo;t build to imagine such spaces. My aim in this article is to give a stastical intuition as to why the angle between two random vectors becomes increasingly likely to be close to 90 degrees, in other words the vectors become more orthogonal, as you increase the number of dimensions of those vectors.\nIn my opinion, the sparsity in terms of angles is a little less intuitive than sparsity in terms of distance between points: I can imagine that as you go from a line, to a square, to a cube, the amount of space (e.g. volume) contained within the object is increasing a lot, so it makes sense that points in high dimensional spaces tend to be further apart.\nComparison of the space covered by a centre point of radius 0.5, in the unit space up to 3D. The proportion of the space that is close to this centre point decreases as dimensions increase. However, the sparsity in terms of angles is less obvious, at least to me. When I imagine putting two random points in a square, and then a cube, it doesn\u0026rsquo;t feel like the distribution of the angle between the points (and the origin) is trending in any obvious direction. This is where viewing the problem statistically can be useful.\nIntuition Using Dice # Mathematically, we express the angle between two vectors using the equation:\n$$ \\cos{\\theta} = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{|\\mathbf{a}| |\\mathbf{b}|} $$\nWhen the two vectors are nearly orthagonal, this means \\(\\cos{\\theta}\\) is nearly 0. If we assume that as we vary our vectors over different dimensions we keep the magnitude of those vectors the same for simplicity, this must mean we are looking to our dot product \\(a \\cdot b\\) to achieve this near 0 behaviour. If we were to choose our vector directions randomly, then this dot product is a summation of random numbers.\nTo more intuitively imagine a summation of random numbers, imagine rolling a dice with values \\([-3,-2,-1,1,2,3]\\), rolling it many times and taking the sum. If you roll it twice, the minimum and maximum total values you can get is is \\([-6,6]\\), and you are reasonably likely to get close to either extreme. However, if you roll the dice 100 times, the min and max total values you can get is \\([-300, 300]\\), however you are likely to get a score (total value) much closer to 0 as most your rolls cancel each other out. The variance proportionally reducing as the number of trials increases is a common and fundamental theme in statistics.\nNote the change in scale along the x-axis (in all graphs) In our original equation for the angle, we divide by the magnitude of the vectors. The simplest analogy in our dice example is to divide by the maximum value achievable using the dice. Therefore, the full \u0026lsquo;game\u0026rsquo; is to roll our dice \\(n\\) times, take the sum, then divide by \\(3n\\). I hope it is intuitive that as the number of rolls increases, more of our rolls as a proportion will be cancelled out by other rolls, and we are more likely to get a final weighted score that is closer to 0.\nAngle Between Vectors # Pairs # Let\u0026rsquo;s now move to viewing the distribution of vectors. As a reminder, what the dot product does is:\n$$ \\mathbf{a} \\cdot \\mathbf{b} = a_1 b_1 + a_2 b_2 + \\cdots + a_n b_n $$\nAnd the equation for the angle is: $$ \\cos{\\theta} = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{|\\mathbf{a}| |\\mathbf{b}|} $$\nTo move from our dice game to the vector space, there are some subtle but not too impactful changes:\nwe compute values in our sum by multiplying two random values from a and b together, instead of just rolling a singular die. (We are ultimately still just picking a random value, symmetrically distributed around 0) we divide by the magnitude of the vector, instead of the magnitude of the best possible rolls (the big picture is both scale the output of the sum to between [-1,1]) When we re-run the experiment using vectors, we unsurprisingly see similar behaviour as we scale up the number of dimensions\nLarger Groups # As pairs of vectors become increasingly more orthoganol as the number of dimensions increases, so do larger groups of vectors. Let\u0026rsquo;s look at the minimum angle between any two vectors in a group, for different size groups in different dimensions.\nApplication in Neural Networks # In modern LLMs, ideas (called features) are represented using vectors. For example, you might have a direction which represents size, so to represent the word \u0026lsquo;huge\u0026rsquo;, the LLM would assign it a vector that is strongly positive in the size direction. Superposition occurs when a single/similar direction in the modelâ€™s embedding space simultaneously represents multiple unrelated features, because the model has to pack many concepts into a limited number of dimensions. For example, if you look at the output of a neuron, it might have similar outputs when representing the idea of \u0026lsquo;size\u0026rsquo; and \u0026lsquo;red\u0026rsquo;, despite those two ideas being quite distinct.\nIt is unclear whether the point represents a lot of redness, size, or a combination This is important in the field of mechanistic interpretability, which aims to try and reverse engineer neural networks into human understandable algorithms. In this field, superposition is a large barrier, because it makes it significantly harder to understand what a neuron activation means when it fires similarly for multiple different ideas (in this case, is the neuron firing because it is thinking about size, or redness?)\nThe goal of this section is to show why the number of near-orthagonal vectors in high dimensional spaces is often used as an explanation to why superposition appears in neural networks.\nA Sense of Scale Using LLaMA 3 # The number of embedding dimensions, the number of dimensions used to encode a word, for modern LLMs tends to increase as the models overall size is scaled. For example, for LLaMA 3, the number of dimensions is:\nNumber of Parameters Number of Dimensions (word embedding) 8 Billion 4,096 70 Billion 8,192 405 Billion 16,384 Let\u0026rsquo;s take a look at the distribution of angles between any two random vectors in the embedding space of LLaMa 3\u0026rsquo;s largest model at 405 Billion Parameters.\nRemember these experiments are just to give an intuition about the sparsity of vectors by looking at random vectors. When LLMs are trained, they can essentially \u0026lsquo;choose\u0026rsquo; which vectors to use (associate with a given feature), meaning they can choose a packing which maximises orthagonality between many vectors. Alternatively, for a given required angle between all vectors, they can maximise the number of vectors they can use.\nMax number of vectors in 2D with minimum angle separation of 72 degrees is 5 You have seen that the average fixed size group of vectors becomes more orthogonal to each other as the number of dimensions grows. In the same manner, the largest possible group you can make with a fixed required angle separation also grows as you increase the number of dimensions. For example, if you wanted to draw as many points on the edge of a circle as possible, where no two points are closer than 45 degrees, this would be less than the number of points you could draw on the surface of a sphere, where no two points are closer than 45 degrees.\nIn fact, the maximum size of a constrained group grows much faster than these graphs might indicate - it is known that the maximum number of near-orthogonal vectors scales exponentially with the number of dimensions.\nThe Kabatiansky-Levenshtein Bound gives an asymptotic formula for approximating this value (technically referred to as the maximal size of a spherical code in n dimensions). We can use this formula to illustrate the approximate shape and magnitude of the function relating the angle separation to the maximum group size for the dimensions used in LLaMA 3, however do not quote these values since the KL bound can become innaccurate for theta values close to 90 degrees.\nMinimum Angle Separation (degrees) Dimensions Approx. Max Number of Vectors 88 16,384 10^20 85 16,384 10^99 80 16,384 10^322 Linking Feature Sparsity and Superposition # As can be observed from the group size graphs earlier, if you increase the group size the average minimum angle between vectors decreases. Therefore, if you want to limit the probability of getting a certain level of similarity between vectors, there is a maximum limit to the group size you can use. In a similar vein, a crucial aspect of the KL bound formula is that the less orthogonality you have between your vectors - in other words the more similar your vectors are - the greater number of vectors you can pack into your space.\nIn deep learning, you want to be able to distinguish different feature vectors, so there is a limit to the allowable similarity between these vectors. If one imagines this allowable similarity as a limited resource, then there are two forces competing for it: a desire to increase the \u0026lsquo;group size\u0026rsquo; (number of simultaneously active features) and a desire to increase the total number of vectors packed into the space (number of total representable features).\nComparison of representations with 2 vs 5 total representable features To further understand why you might not be able to have as many simultaneously firing feature vectors, consider that the output of the neurons will be a point in the embedding space. In the first case, with just \u0026lsquo;red\u0026rsquo; and \u0026lsquo;size\u0026rsquo;, you can take any point and reverse engineer the vectors that were combined to make it. However, in the case where you have 5 total features and I show you a point that is a combination of two of them, you might struggle to confidently say for some points which vectors were \u0026lsquo;combined\u0026rsquo; to get that point:\nTherefore, the model has to learn a trade-off in its representation. For example, consider how the two models above might represent a car, or a ladybird.\nTotal Known Features Number of Active Features (group size) Car Representation Ladybird Representation Size, Red 2 Fairly big, not very red small, very red Size, Red, Vehicle, Beauty, Animal 1 Definitely a vehicle very red? In reality, the model doesn\u0026rsquo;t necessarily \u0026ldquo;choose\u0026rdquo; to only activate one feature, for example only representing a ladybird with \u0026ldquo;very red\u0026rdquo;. It will have detectors for \u0026lsquo;redness\u0026rsquo;, \u0026lsquo;size\u0026rsquo; and \u0026lsquo;animal\u0026rsquo;, and if it was given a ladybird it would signal strongly for all of them. Instead, the model has to learn features that are both useful and don\u0026rsquo;t often appear all at once in the training dataset. For instance, if the training set contained many instances of ladybirds, the model learning 5 features may want to choose more specific and independent features. Alternatively, if there were far more instances of ladybirds than cars, another solution could be to remove the \u0026lsquo;vehicle\u0026rsquo; feature completely, to better focus on the remaining, more important features.\nThis is why the sparsity of features is a useful predictor for the amount of superposition in a neural network. \u0026lsquo;Sparsity of features\u0026rsquo; essentially means how often multiple features are active simultaneously as a proportion of the total identifiable features. Language can represent a vast array of ideas, but only a very small portion of those are present in a given word or sentence (or even LLM context window). This means the balance between the two fighting forces tips towards the total number of representable features, and as discussed before in order to be able to increase total features by packing more vectors into the space then you must accept more similar vectors for those features.\nGreater similarity between feature vectors manifests as superposition, therefore language models like GPT pose a major challenge to interpretability researchers.\nConclusion and Further Reading # High dimensional spaces can seem intimidating, however often this stems from trying to force yourself to think about problems geometrically, when they might be easier understood through another lens. Having a good conceptual understanding of such spaces can help understand modern neural networks, since modern applications are centred around transformers that use high dimensionality embeddings. These models \u0026rsquo;think\u0026rsquo; in high dimensionality spaces, so to understand the models it would seem useful to also understand the characteristics of how their thoughts are represented.\nUnderstanding how the angle between vectors can affect feature representations is just the tip of the iceberg when it comes to understanding neural networks. I hope to gain a further grasp of other fundamental concepts that constrain and affect neural networks, especially when applied to mechanistic interpretability, and I would encourage others too to explore the topic. If you\u0026rsquo;re interested in mechanistic interpretability or AI safety more broadly, then Anthropic\u0026rsquo;s research is a great place to start:\nToy Models of Superposition - an in-depth exploration of superposition (a great follow up from this post) Alignment Faking in Large Language Models - short and non-technical example of how modern LLM\u0026rsquo;s can strategically lie (the perfect introduction to AI safety) ","date":"16 May 2025","externalUrl":null,"permalink":"/problems/vector_orthogonality_and_llm_superposition/","section":"Problems","summary":"If you\u0026rsquo;re interested in experimenting yourself with the topics in this post, you may want to check out the code I used to generate all the graphs and stats in the accompanying jupyter notebook.","title":"High Dimension Vector Orthogonality and Superposition in LLMs","type":"problems"},{"content":"","date":"16 May 2025","externalUrl":null,"permalink":"/tags/maths/","section":"Tags","summary":"","title":"Maths","type":"tags"},{"content":"This section contains all my articles on Maths and Algorithms problems.\n","date":"16 May 2025","externalUrl":null,"permalink":"/problems/","section":"Problems","summary":"This section contains all my articles on Maths and Algorithms problems.","title":"Problems","type":"problems"},{"content":"Sequences and Time-Series have many applications. The primary inspiration for this project is my experience solving problems in programming with a strong mathematical component, such as those found on Project Euler, as well as my desire to get more experience implementing high-performance, concurrent applications in C++.\nOften these problems that fuse concepts from Discrete Maths with programming can be solved using state-based models like markov chains. You can see an example of this in Leetcode Problem 552, and I plan on writing solutions using this approach to more complex problems soon.\nThe project is currently in its infancy so I haven\u0026rsquo;t made it public yet, but it will be an open source tool for tackling these problems; while there already exist solutions for automatically generating markov chains to fit a sequence, they require the user to preprocess the sequence to the point that it can be represented using a succint markov chain. This project aims to broaden the capabilities of existing tools to move some of this burden of preprocessing and understanding off the user, and produce a model that is easily implemented in a performance-critical environment.\nFor example, many of the problems I have come across have properties where successive numbers in a sequence can be grouped together into blocks, and it is far more useful to analyse the sequence of these blocks rather than the sequence itself. This tool would allow the user to suggest a range of block sizes to try, fit models using these different sizes, and then provide a commentary on how well these different models fit.\nOverall, this project aims to improve on existing solutions in the following ways:\nmore tools for automating preprocessing and assessing models greater performance when generating models simpler models with fewer states reducing \u0026lsquo;black boxes\u0026rsquo; increasing model performance ","date":"10 February 2025","externalUrl":null,"permalink":"/projects/sequence_analyser/","section":"Projects","summary":"Sequences and Time-Series have many applications.","title":"Active: Markov Sequence Analyser","type":"projects"},{"content":"This section contains updates and descriptions of projects I\u0026rsquo;m working on. I\u0026rsquo;ve only just started the blog, so hang around to watch this page get filled!\n","date":"10 February 2025","externalUrl":null,"permalink":"/projects/","section":"Projects","summary":"This section contains updates and descriptions of projects I\u0026rsquo;m working on.","title":"Projects","type":"projects"},{"content":" The Lesson # It\u0026rsquo;s easy to get hooked on the idea of performance and optimisation, but when it comes to recurrence relation problems in programming, it\u0026rsquo;s easy to fall into the trap of chasing the perfect solution. You\u0026rsquo;ve done your homework, read up on all the maths, and now you\u0026rsquo;re ready to blast the next recurrence relation problem you see with algebra, find and utilise the closed form solution, and leave other\u0026rsquo;s code in the dust.\nThere\u0026rsquo;s only one problem: most of the time, it doesn\u0026rsquo;t work.\nFor starters, existing general purpose solutions like matrix exponentiation already work well: it\u0026rsquo;s simple to implement, applies to many recurrence problems and finds solutions in logarithmic time, and while I hate to admit it, in most situations that is fast enough.\nIndeed, most of the time you would actually care about improving speed beyond logarithmic time is when the inputs are really large, but this means that you are almost certainly taking the modulus of the answer, at least in any programming challenge, because most recurrence relations grow fast.\nTake for example the fibonacci sequence, the million\u0026rsquo;th term in the sequence has over 200,000 digits. These large numbers are awkward to deal with, so programming challenges ask for answers to be given to a modulus.\nThis use of modular arithmetic makes implementing closed form solutions much harder, because this solution will often involve real numbers instead of just integers, and the normal rules of modular arithmetic do not apply when using real numbers. You will see later in the problem I tried just how pesky these real numbers can be.\nHowever, there may still be those out there who would be undeterred by these barriers, who want to push on for greater performance even when it might not seem strictly necessary. Unfortunately, the real killer to the fervent optimser\u0026rsquo;s dream is the real-world performance of these solutions: often they actually perform worse than easier methods like matrix exponentiation.\nThis is partially because at the end of the day you\u0026rsquo;re doing exponentiation, either with scalars in the closed form or with matrices, meaning the extent to which you can improve the overall time complexity is limited. However, what can actually make the recurrent solution slower is the requirement for the real numbers in the closed form solution to be extremely accurate for large inputs, meaning finding and using these numbers can be extremely computationally expensive.\nYou will now see how I tried to solve a fairly standard recurrence relation problem using a more mathematical approach than is traditional, and how it failed.\nA Failed Attempt at getting Mathematical # The problem is one I have tackled before: Leetcode Problem 552. If you want more information about the problem and the solution using matrix exponentiation then please go read that article; the crux of the problem, if we want to use recurrence relations, is finding the following: $$ v(n) = \\sum_{i=0}^{n-1} F(i)F(n-i-1) $$ where \\(F(n)\\) is a Tribonacci number: $$ F(n)=F(n-1)+F(n-2)+F(n-3) $$\nMathematical Solution # Calculating F(n) # First, I found the roots of the characteristic equation: $$ x^3-x^2-x-1=0 $$\nI then used sympy to find the roots and coefficients of the closed form solution: $$ F(n) = A r_1^n + B r_2^n + C r_3^n $$\nHere are the numerical approximations to these values:\n\\(A \\approx 1.137 \\) \\(r_1 \\approx 1.839 \\) \\(B \\approx -0.0687 + 0.124 i \\) \\(r_2 \\approx -0.420 + 0.606 i\\) \\(C \\approx -0.0687 - 0.124 i\\) \\(r_3 \\approx -0.420 - 0.606 i\\) The exact roots and coefficients are complicated, but what is important about these values are:\nthey aren\u0026rsquo;t rational two of the terms in our function are complex the two complex terms are very small and get smaller as n grows when combined they make an integer (since \\(F(n)\\) is an integer) The small size of the two complex terms are very helpful, because they are small enough that we can simply ignore them and round the first term: $$ F(n) \\approx A r_1^n $$\nCalculating v(n) # Remember we have the following formula for \\(v(n)\\): $$ v(n) = \\sum_{i=0}^{n-1} F(i)F(n-i-1) $$\nIf we now sub in our formula for \\(F(n)\\)\n$$ v(n) = \\sum_{i=0}^{n-1} (Ar_1^i+Br_2^i+Cr_3^i)(Ar_1^{n-i-1}+Br_2^{n-i-1}+Cr_3^{n-i-1}) $$\nWe only need to consider terms involving \\(Ar_1^n\\) in our approximation: $$ v(n) \\approx \\sum_{i=0}^{n-1} A^2r_1^{n-1} + Ar_1^i(Br_2^{n-i-1}+Cr_3^{n-i-1}) + Ar_1^{n-i-1}(Br_2^i+Cr_3^i) $$\nThe sum of terms 2 and 3 are the same: $$ v(n) \\approx \\sum_{i=0}^{n-1} A^2r_1^{n-1} + 2Ar_1^i(Br_2^{n-i-1} + Cr_3^{n-i-1}) $$\nWe can use the following formula to help us evaluate the 2nd term: $$ \\sum_{i=0}^n a^i b^{n-i} = \\frac{b^{n+1}-a^{n+1}}{b-a} $$\nIn this case, due to the size of the complex terms growing insignificant when raised to the power of n, we can ignore the \\(a^{n+1}\\) part for those terms: $$ v(n) \\approx nA^2r_1^{n-1} + 2A(B \\frac{r_1^n}{r_1-r_2} + C \\frac{r_1^n}{r_1-r_3}) $$\nWe are only interested in the real part of the sum, and we know that the answer we are approximating is real, so we can safely remove the imaginary part (it would have been cancelled out by the other complex terms if we hadn\u0026rsquo;t ignored them). The real part is the same for both terms in the brackets, so we can just take the real part of one and multiply by two: $$ v(n) \\approx nA^2r_1^{n-1} + 4Ar_1^n \\cdot \\Re(\\frac{B}{r_1-r_2}) $$\nFinal formula # The actual solution requires adding \\(F(n)\\) to \\(v(n)\\) and taking the modulus: $$ \\text{solution} = \\text{round}(Ar_1^{n-1}[nA+4r_1 \\cdot \\Re(\\frac{B}{r_1-r_2})+r_1]) \\mod m $$\nThis formula looks a little complicated, but there are only two parts of the formula that involve n. Therefore we can simply the formula to: $$ \\text{solution} = \\text{round}(r_1^{n-1}[A^2 n + c]) \\mod m $$\nNote that while the \u0026lsquo;round\u0026rsquo; function may look a little messy, it only accounts for the complex terms in the closed form solution that we ignored, which get very small quickly, so even without the round function the answer would be very close, especially for larger n.\nImplementing the solution # This is the point where maths meets reality. At this point a mathematician would have sat back and enjoyed their formula that gives the answer for any n neatly.\nHowever, this is very difficult to implement in a program due to the fact that \\(r_1\\) is a real number, and so are the other terms \\(A^2 n + c\\), which stops us from being able to do any useful modular arithmetic tricks.\nLet\u0026rsquo;s explore some of the ideas I tried to use modular arithmetic to get a fast solution.\nmodular exponentiation # Normally, you would see \\(r^n \\mod m\\) and think of using the multiplicative order of r: $$ r^k = 1 \\mod m $$ $$ r^n = r^{n \\ \\% \\ k} \\mod m $$\nUnfortunately, this does not hold when using real numbers. For example:\nlet \\(k = \\log_{1.8}(8)\\) so \\(1.8^k = 1 \\mod 7\\) \\(1.8^{10} \\approx 0.05 \\mod 7 \\) \\(1.8^{10 \\ \\% \\ k} \\approx 5.8 \\mod 7\\) mixed approach with matrices # OK, so at this point I thought fine, if I can\u0026rsquo;t find \\(r_1^{n-1} % m\\) using my closed form solution, I can just find the related value \\(F(n-1)\\) using matrix exponentiation, and then multiply by \\(A^2 n + c\\).\nThis should still be faster than finding the answer directly with matrix exponentiation since to find the solution directly using that technique requires a 7x7 matrix (in my solution), while to just find \\(F(n)\\) only requires a 3x3 matrix. Since matrix multiplication is \\(O(n^3)\\), this should result in roughly a 12x speed increase.\nUnfortunately, once again the rules of modular arithmetic thwart us. In particular, while the following property holds for integers, it is not true for real numbers: $$ ab \\mod m \\ne ((a \\mod m) \\cdot (b \\mod m)) \\mod m $$\nThis means that even if we use matrix exponentiation to calculate \\( r_1^{n-1}\\) mod m, we cannot multiply it by our constant to get the answer mod m.\nNo modular tricks # So we can\u0026rsquo;t use any modular tricks, so what? We can still use our formula to calculate the final value and then just mod that.\nThis approach works and can be easily implemented:\ndef solve_with_recurrence(n, A, B, r1, r2): if n == 1: return 3 elif n == 2: return 8 c = A*(4*r1*sympy.re(B/(r1-r2)) + r1) return round(r1**(n-1) * (A*A*n+c)) % 1000000007 Analysing my solution # However, what\u0026rsquo;s being hidden in the previous code snippet is the calculation of the constants passed to the function, as well as exactly what they look like. The problem is that since our answer could be thousands of digits long, so do our floats. Doing calculations with these large floats is computationally expensive, but what is even more computationally expensive is calculating them in the first place.\ndef get_constants(precision): # get precomputed exact representations of the values with open(\u0026#34;sympy_expr.txt\u0026#34;, \u0026#34;r\u0026#34;) as f: stored_expr = sp.sympify(f.read()) return [term.evalf(precision) for term in stored_expr] timeit.timeit(lambda: get_constants(digits_precision), number=5)/5 digits precision time 1,000 0.09s 5,000 1.34s 10,000 4.6s 20,000 16s 100,000 5m 15s It takes a long time to compute these values, seemingly exhibiting a time complexity of ~\\(O(n^2)\\), but surely with thousands of places of precision we should be able to calculate solutions for massive n? Well, I found through experimentation that in order to maintain accuracy, you need roughly \\(\\frac{n}{3}\\) places of decimal precision to calculate the solution for n. This means that this method quickly becomes infeasible for large n.\nHowever, let\u0026rsquo;s say we can precompute these values to a large precision on our supercomputer, we are then guaranteed our function will work up to a certain n and any normal computer can compute solutions blazingly fast; it won\u0026rsquo;t fix the fact that our program now has a linear space complexity but as long as it\u0026rsquo;s fast, it\u0026rsquo;s fine. Let\u0026rsquo;s take a look at how our solution using the closed form solution stacks up against the default matrix exponentiation solution:\nA,B,r1,r2 = get_constants(n//3) recurrence_time = timeit(lambda: solve_with_recurrence(n,A,B,r1,r2), number=1000)/1000 matrix_time = timeit(lambda: solve_with_matrix(n), number=1000)/1000 n matrix time recurrence time 50,000 \u0026lt;0.1ms 5ms 100,000 \u0026lt;0.1ms 11ms 300,000 0.1ms 75ms \\(10^{61}\\) 1ms ? It turns out the function, even with precomputed values, is actually much slower than the matrix exponent method due to the precision of the floats and the size of the intermediate numbers being calculated, since we cannot make use of any modular arithmetic tricks. In fact, due to the precision of the floats growing linearly, the new function displays a completely different (worse) time complexity.\nConclusion # There\u0026rsquo;s a lot of smart engineers and mathematicians out there that can probably improve on my attempts using the closed form solution to the recurrence and leverage its power to some extent.\nHowever, I\u0026rsquo;d prefer to not have to be smart. If nothing else from my little journey, please take away the fact that you can spend a significant amount of time playing around with the maths and code to try and find a way to make your fancy mathematical method work, all for it to be no faster than what you could have done in 10 minutes.\nSometimes it pays to try and be clever, but other times it\u0026rsquo;s best to just use what\u0026rsquo;s well established. Sure, go ahead and check what the closed form solution is, but if you see irrational complex numbers, I\u0026rsquo;d recommend spending your time optimising another problem.\n","date":"30 January 2025","externalUrl":null,"permalink":"/problems/recurrence_relations_in_programming/","section":"Problems","summary":"The Lesson # It\u0026rsquo;s easy to get hooked on the idea of performance and optimisation, but when it comes to recurrence relation problems in programming, it\u0026rsquo;s easy to fall into the trap of chasing the perfect solution.","title":"Pragmatic Recurrence Relations: Why the Closed Form Won't Help You","type":"problems"},{"content":"","date":"30 January 2025","externalUrl":null,"permalink":"/tags/recurrence-relation/","section":"Tags","summary":"","title":"Recurrence Relation","type":"tags"},{"content":" Problem Description # Two random points, one red and one blue, are chosen uniformly and independently from the interior of a square. To ten decimal places, what is the probability that there exists a point on the side of the square closest to the blue point that is equidistant to both the blue point and the red point?\nYou can check out the problem on their website, however if you want to check their solution be aware that as of writing there is a mistake in their proposed integral, so please refer to the one I have provided. I have made them aware of this issue.\nSolution # Mathematical representation of the problem # For this problem, we need to consider which parts of the selected wall are closest to blue, closest to red, and an equal distance.\nLet\u0026rsquo;s plot some examples, highlighting the part of the relevant side which is closest to red or blue:\nAs seen above, for any two points, there are two possibilites:\none point is closest to the whole side each point has a share of the side If one point is closest to the whole side, then there cannot be a point on that side which is equidistant from both. However, in the 2nd case, there must be a point on the side equidistant of the two, located where the two highlighted areas meet.\nTo check these cases, we check the two corners. If the closest point to each corner is the same, then that point is closest to the whole side, but if one corner is closest to blue and the other closest to red, then each point has a share of the side and there will be a solution.\nLet\u0026rsquo;s fix the blue point, and consider its distance from each corner:\nThe red point needs to closer to one corner, and not the other. We can find the boundary for being closer to a corner by drawing a circle from it:\nTo get a solution the red point must be inside one circle, but not both:\nRecap # We have found an alternative way to represent the original problem as the probability of the red point being in the highlighted red area. From now on we will define the total area of the square to be 1, so if the red area is 0.63 then the probability of there being a valid solution is 63%.\nThe question asks about the average probability across all possible blue point positions, so the answer to the question is the average red area.\nCreating an equation to solve # simplifying the representation # Due to the symmetry of the square, we can choose to view the blue point from the perspective where its closest side is along the x-axis, and its closest corner is along the y axis.\n(cyan = possible place for the blue point)\nProbability for a single blue position # To find the area, we can add the areas of the individual quarter-circles, and then subtract their intersection twice (one time to account for double counting, and the second to actually remove the area from the valid space).\nLet \\(r_c,r_f\\) be the distance from the blue point to the closest and furthest corner of its side respectively.\nLet \\(p(x,y)\\) be the probability of a valid solution given the blue point is at position \\((x,y)\\).\n$$ p(x,y) = \\frac{\\pi}{4}(r_c^2+r_f^2) - 2*\\text{intersection} $$\nTo find the area of intersection, we add together the segments from each circle that makes it, and subtract the total area formed by the triangle between the two corners and \\((x,y)\\). $$ \\text{intersection} = \\frac{\\alpha}{2 \\pi} \\pi r_c^2 + \\frac{\\beta}{2 \\pi} \\pi r_f^2 - \\frac{y}{2} $$\nwhere $$ \\alpha = \\arctan(\\frac{y}{x}) $$ $$ \\beta = \\arctan(\\frac{y}{1-x}) $$ $$ r_c = \\sqrt{x^2+y^2} $$ $$ r_f = \\sqrt{(1-x)^2+y^2} $$\nmean probability for all blue positions # To get the average of our probability function for all blue points, we integrate over all blue points and divide by the area of the bounds.\nThe bounds on the blue point\u0026rsquo;s position are represented by the cyan triangle seen earlier. This triangle has a slope that follows the gradient \\(y=x\\), so we represent the triangle using the following inequalities: $$ 0 \\le y \\le x $$ $$ 0 \\le x \\le 0.5 $$\nThe area of these bounds is \\(\\frac{1}{8}\\), so the mean probability (and the answer to the question) is given by: $$ 8\\int_{0}^{\\frac{1}{2}} \\int_{0}^{x} p(x,y) \\ dx \\ dy $$\nSolving the equation # Given that we don\u0026rsquo;t need to find the exact solution to the problem, we can use numerical methods to approximate the integral:\nfrom scipy.integrate import dblquad from math import atan, pi def p(y,x): if x==0 or y==0: return 0 r1_sq = x**2 + y**2 r2_sq = (1-x)**2 + y**2 area = pi / 4 * (r1_sq+r2_sq) intersection = 1/2*atan(y/x)*r1_sq + 1/2*atan(y/(1-x))*r2_sq - y/2 return area - 2*intersection integral_val= dblquad(p, 0.0, 0.5, lambda x: 0.0, lambda x: x)[0] result = integral_val*8 This outputs 0.49140757883830793\nHowever, we can do better! Using python\u0026rsquo;s sympy library, we can calculate the exact result of the integral:\nimport sympy as sp x,y = sp.symbols(\u0026#39;x y\u0026#39;) r1_sq = x**2 + y**2 r2_sq = (1-x)**2 + y**2 area = sp.pi / 4 * (r1_sq+r2_sq) intersection = sp.Rational(1,2)*(sp.atan(y/x)*r1_sq + sp.atan(y/(1-x))*r2_sq - y) integrand = area - 2*intersection integral_val = sp.integrate(integrand, (y,0,x), (x,0,sp.Rational(1,2)) result = integral_val*8 This outputs -17*log(2)/6 + 1/12 + pi/6 + 4*log(4)/3\nWe can simplify this further:\nsimplified_log = sp.simplify(-17 * sp.log(2) / 6 + 4*sp.log(4)/3) # -log(2)/6 Therefore the final answer is: $$ \\frac{1+2\\pi - \\ln(4)}{12} $$\nBonus: An easy mistake when using sympy # When I first wrote the previous code, I used 0.5 instead of sp.Rational(1,2) as the bound for the integral:\nintegral_val = sp.integrate(integrand, (y,0,x), (x,0,0.5)) This is a mistake because it prevented sympy from giving an exact answer, however it still tried, leading to a potentially misleading output that mixes exact terms with numerical approximations.\nresult # -2*log(2)/3 + 0.125*pi + 0.560806617512881 However, you can still use evaluate the output to get the correct numerical approximation:\nresult.evalf()*8 # 0.491407578838308 ","date":"22 January 2025","externalUrl":null,"permalink":"/problems/jane_street_and_numerical_integration/","section":"Problems","summary":"Problem Description # Two random points, one red and one blue, are chosen uniformly and independently from the interior of a square.","title":"Jane Street Puzzle: Beside The Point","type":"problems"},{"content":"","date":"17 June 2024","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"17 June 2024","externalUrl":null,"permalink":"/categories/leetcode/","section":"Categories","summary":"","title":"Leetcode","type":"categories"},{"content":" Problem Description # Check out the problem on Leetcode\nGiven a non-negative integer \\(c\\), decide whether there exists two integers \\(a\\) and \\(b\\) such that \\(a^2+b^2=c\\)\nSolution # Try different combinations of \\((a,b)\\), adjusting the guess each time based on whether their sum is too large or too small. I define \\(a\\) as the smaller value and \\(b\\) as the larger value.\nInitial values # \\(a=0\\) \\(b^2\u0026gt;=c\\) \\(\\text{sum}=b^2\\) Finding the solution # If the sum is too big, then make \\(b\\) smaller. If it is too large, make \\(a\\) larger. Stop when \\(a\u0026gt;b\\).\nOptimising sum calculation # \\((a+1)^2 = a^2+2a+1\\)\n\\((b-1)^2 = b^2-2b+1\\)\nWe can use this to update sum:\n\\((a+1)^2+b^2 = a^2+2a+1+b^2\\)\n\\( \\quad \\quad \\quad \\quad \\quad \\ \\ \\ = \\text{sum} + 2a + 1\\)\nTherefore, instead of recalculating the sum each time using a*a+b*b, we can instead use these formulae. For example, if increasing a, sum += 2a+1.\nComplexity # Time complexity: O(\\(\\sqrt{n}\\) )\nSpace complexity: O(1)\nCode # bool judgeSquareSum(const int\u0026amp; c) { // let a\u0026lt;=b unsigned int a = 0; unsigned int b = ceil(sqrt(c)); unsigned int sum = b*b; while(a\u0026lt;=b){ if(sum==c){ return true; } else if(sum\u0026lt;c){ // increase a // (a+1)^2 = a^2 +2a+1 sum += 2*a+1; a++; } else{ // decrease b // (b-1)^2 = b^2-2b+1 sum += 1-2*b; b--; } } return false; } ","date":"17 June 2024","externalUrl":null,"permalink":"/problems/leetcode_633/","section":"Problems","summary":"Problem Description # Check out the problem on Leetcode","title":"Leetcode 633: Sum of Square Numbers","type":"problems"},{"content":" Problem Description # Check out the problem on Leetcode\nYou have a string representing the attendance record of a student:\nA = Absent L = Late P = Present You must find the number of strings of length n that have the following properties:\nonly contain the letters (A,L,P) contain less than two A\u0026rsquo;s don\u0026rsquo;t contain any occurences of \u0026lsquo;LLL\u0026rsquo; Solution 1 - Using Recurrences # This approach will be to use recurrence relations to calculate the number of combinations whilst avoiding considering the actual possible strings.\nCreating a recurrence (ignoring absences) # Let \\(S_n\\) be a valid string with no absences. Let \\(F(n)\\) be the number of such strings. We can make a valid string \\(S_n\\) using shorter ones:\n\\(S_{n-1}\\) + P \\(S_{n-2}\\) + PL \\(S_{n-3}\\) + PLL We can get all possible strings (longer than 3) this way with no double counting. For example, PLLPPL = PLL + P + PL.\nTherefore: \\(F(n)=F(n-1)+F(n-2)+F(n-3)\\)\nAccounting for absences # We are allowed to have a maximum of one absence.\nIf the absence is after lesson \\(i\\), then we have a no-absence string of length \\(i\\) before it and a no-absence string of length \\(n-i-1\\) after it. For example, if we have 10 lessons and the student is absent for lesson 3:\n$$ \\text{string} = S_2 + A + S_7 $$\nTherefore, the number of valid strings for a given absence = \\(F(i) * F(n-i-1)\\)\nWe can find the total number of valid strings by iterating over all possible lessons up to \\(i=n/2\\) and then multiply by two because \\(F(2)*F(7) = F(7)*F(2)\\).\nFinally, we add \\(F(n)\\) to the total, for the situation where the student has no absences.\nComplexity # Time complexity: \\(O(n)\\) Space complexity: \\(O(n)\\) Code # int checkRecord(const int\u0026amp; n) { // check for base case if(n==1){ return 3; } else if(n==2){ return 8; } // F[n] = number of ways to form a string of length n with no absences vector\u0026lt;long long\u0026gt; F(n+1, 0ll); F[0]=1; F[1]=2; F[2]=4; const long long MOD = 1000000007; for(int i=3; i\u0026lt;=n; i++){ F[i] = (F[i-3]+F[i-2]+F[i-1]) % MOD; } long long numWithAbsences = 0; for(int i=0; i\u0026lt;n/2; i++){ numWithAbsences += (F[i]*F[n-i-1]) % MOD; } numWithAbsences *= 2; if(n % 2 == 1){ numWithAbsences += F[n/2]*F[n/2]; } return (F[n] + numWithAbsences) % MOD; } Solution 2 - Matrix Exponentiation # Often in questions where you have a limited number of states you can be in, and you\u0026rsquo;re trying to find the total number of ways to generate some sequence of these, you can represent this using a transition matrix.\nThe \u0026lsquo;state\u0026rsquo; of our string in this case depends on the last letters and whether the student has been absent before. In our transition matrix, we will put a \\(1\\) in position \\((x,y)\\) if we can move from state \\(x\\) to state \\(y\\). We will represent a state where we have already been absent using \\(x^A\\).\nGraphic by user5382x on Leetcode To understand this matrix, lets look at row \\(L^A\\). This means the last letter is \\(L\\) and the student has been absent before. Therefore, the next day can either be another late day, in which case we move to state \\({LL}^A\\), or the student is present, in which case we move to \\(P^A\\). No other states are possible, so the rest of the row values are 0.\nusing the matrix # This matrix represents all the possible letters you can add in one day, depending on your state. If we want to find the number of strings we can add in \\(n\\) days, then we can just multiply \\(n\\) of these matrices together (add one letter, then another, then another\u0026hellip;)\nWe can find the exponent of the matrix in \\(O(\\log n)\\)using exponentiation by squaring. Essentially, we keep squaring the matrix \\(M\\) so we get \\([M, M^2, M^4, M^8, M^{16} \\dots ]\\). Since this question uses modular arithmetic, we will find these values mod m.\nWe multiply values in the list together to get our exact exponent. For example, if we wanted to find \\(M^{13}\\), we would use \\(M^8 \\cdot M^4 \\cdot M\\).\nOnce we have our final transition matrix, we want to sum together all the different paths that start with the possible starting states. The mathematical way to write this is to first get the dot product of our starting states [1,1,1,0,0,0,0] (since we must start with an A,P or L) with our final transition matrix, which will tell us how many ways there are to get to each final state.\nFor example, if we calculate the dot product of our starting states with \\(M^5\\), we get [13,13,7,4,30,19,8], which means that there are 19 ways of reaching the state \\(L^A\\).\nTherefore, to get the total number of paths to any final state, we take the sum of these numbers.\nCode # I used numpy to calculate the matrix operations efficiently.\ndef solve_with_matrix(n): base = np.array([ [0, 0, 0, 0, 1, 1, 0], [1, 1, 1, 0, 0, 0, 0], [1, 1, 0, 1, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 1, 0], [0, 0, 0, 0, 1, 0, 1], [0, 0, 0, 0, 1, 0, 0] ], dtype=np.int64) final_transition_matrix = np.eye(7, dtype=np.int64) MOD = 10**9+7 n = n-1 while(n\u0026gt;0): if(n%2==1): final_transition_matrix = np.dot(final_transition_matrix, base)%MOD base = np.dot(base, base) % MOD n //= 2 start = np.array([1,1,1,0,0,0,0], dtype=np.int64) return np.dot(start, final_transition_matrix).sum() % MOD Complexity # Time Complexity: \\(O(\\log n)\\) Space Complexity: \\(O(1)\\) ","date":"26 May 2024","externalUrl":null,"permalink":"/problems/leetcode_552/","section":"Problems","summary":"Problem Description # Check out the problem on Leetcode","title":"Leetcode 552: Student Attendance Record II","type":"problems"},{"content":" Problem Description # Check out the full problem description\nThere are two types of people:\nThe good person: The person who always tells the truth. The bad person: The person who might tell the truth and might lie. You are given a 0-indexed 2D integer array statements of size n x n that represents the statements made by n people about each other. More specifically, statements[i][j] could be one of the following:\n0 which represents a statement made by person i that person j is a bad person. 1 which represents a statement made by person i that person j is a good person. 2 represents that no statement is made by person i about person j. Additionally, no person ever makes a statement about themselves. Formally, we have that statements[i][i] = 2 for all 0 \u0026lt;= i \u0026lt; n.\nReturn the maximum number of people who can be good based on the statements made by the n people.\nSolution # The general approach I will take is to iterate over each possible configuration of good and bad people using bit manipulation.\nRepresenting a configuration # Represent a possible configuration of n people using n bits.\n1 = good person 0 = bad person E.g. 1010 = People 1,3 are good. People 2,4 are bad.\nIterating over configurations # Naive approach # We can iterate over all the possible configurations, starting with all people lying, like so:\nfor(int config=0; config\u0026lt;(1\u0026lt;\u0026lt;n); config++){ // check if the solution is valid } Improved approach # We first start by looking at configurations where everyone is good. We then stop looking when none of the remaining possible solutions can beat our current one.\nFor example, imagine we have five people, and found a configuration where 3 could be telling the truth (maxGood is 3), we need to find at least four 1\u0026rsquo;s (truthful bits) to find a better solution. In this example, if config\u0026lt;01111, then we can stop because config is decreasing, so it will never have four 1\u0026rsquo;s.\ninitialise config = 111...1111 stop searching when config \u0026lt; ..000111.. (ending in maxGood 1\u0026rsquo;s) for(int config=(1\u0026lt;\u0026lt;n)-1; config \u0026gt;= (1\u0026lt;\u0026lt;(maxGood+1))-1; config--){ // check if the solution is valid } Checking if a solution is valid # A solution is invalid when someone who is assumed to be truthful tells a lie. (Ignore what the liars say.)\nTherefore, iterate over all the truthful people in a solution and check what they say matches our proposed solution.\nbool configurationWorks = true; // iterate over each person in the config for(int j=0; j\u0026lt;n \u0026amp;\u0026amp; configurationWorks; j++){ if((config\u0026gt;\u0026gt;j)\u0026amp;1){ // the current person is telling the truth for(int h=0; h\u0026lt;n; h++){ // the accused is not what they are supposed to be // e.g. person j says person h is good when they are bad if(statements[j][h] != 2 \u0026amp;\u0026amp; statements[j][h] != ((config\u0026gt;\u0026gt;h)\u0026amp;1)){ configurationWorks = false; break; } } } } Setting the new Max Score # The number of good people in a configuration is the number of 1\u0026rsquo;s in the binary representation of config. There is already a function to do this called __builtin_popcount.\nif(configurationWorks){ maxGood = max(maxGood, __builtin_popcount(config)); } Complexity # Time complexity: O(\\(N^2 * 2^N\\))\nSpace complexity: O(\\(\\sqrt{N}\\))\nCode # class Solution { public: int maximumGood(const vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; statements) { int n = statements.size(); int maxGood = 0; // config has n bits. A 1 represents good, and a 0 represents bad for(int config=(1\u0026lt;\u0026lt;n)-1; config \u0026gt;= (1\u0026lt;\u0026lt;(maxGood+1))-1; config--){ bool configurationWorks = true; // iterate over each person in the config for(int j=0; j\u0026lt;n \u0026amp;\u0026amp; configurationWorks; j++){ if((config\u0026gt;\u0026gt;j)\u0026amp;1){ // the current person is telling the truth for(int h=0; h\u0026lt;n; h++){ // the accused is not what they are supposed to be // e.g. person j says person h is good when they are bad if(statements[j][h] != 2 \u0026amp;\u0026amp; statements[j][h] != ((config\u0026gt;\u0026gt;h)\u0026amp;1)){ configurationWorks = false; break; } } } } if(configurationWorks){ maxGood = max(maxGood, __builtin_popcount(config)); } } return maxGood; } }; ","date":"24 May 2024","externalUrl":null,"permalink":"/problems/leetcode_2151/","section":"Problems","summary":"Problem Description # Check out the full problem description","title":"Leetcode 2151: Maximum Good People Based on Statements","type":"problems"},{"content":" Problem Description # The Problem:\nGiven an integer array nums that does not contain any zeros, find the largest positive integer k such that -k also exists in the array.\nReturn the positive integer k. If there is no such integer, return -1.\nExample:\nInput: nums = [-1, 10, 6, 7, -7, 1] Output: 7 Explanation: Both 1 and 7 have their corresponding negative numbers in the array. 7 is the largest. Solution Approach # General Approach # Pass over the array, storing the fact we have seen that item. If the number\u0026rsquo;s negative has been seen before, then update the new maximum value.\nOptimisations # The constraints only allow a small range of possible values:\n\\( -1000 \u0026lt;= \\text{nums[i]} \u0026lt;= 1000\\)\nTherefore, we can use a static data structure, like an array, and store for every possible number whether we have seen it.\nSince we only need to store true/false for each number to represent whether it has been seen before, we can use a bitset for space efficiency.\nGiven the smallest possible value is -1000, this will be index 0, and so to get the index of a number you add 1000.\nExample # we see the number 47 seen[1000-47]==1 is checked and found false so seen[1047] is set to 1 Later, the number -47 is seen. seen[1000 -(-47)]==1 is checked and found true |-47| is compared to the current maximum number Complexity # Time complexity: O(n)\nSpace complexity: O(n)\nCode # class Solution { public: int findMaxK(const vector\u0026lt;int\u0026gt;\u0026amp; nums) { // indexes 1001-2000 represent positives. 0-999 are negatives bitset\u0026lt;2001\u0026gt; nums_bitset; int ans = -1; for(const int\u0026amp; num : nums){ if(nums_bitset[1000-num]==1){ ans = max(ans, abs(num)); } else{ nums_bitset[1000+num] = 1; } } return ans; } }; ","date":"2 May 2024","externalUrl":null,"permalink":"/problems/leetcode_2441/","section":"Problems","summary":"Problem Description # The Problem:","title":"Leetcode 2441. Largest Positive Integer That Exists With Its Negative","type":"problems"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]