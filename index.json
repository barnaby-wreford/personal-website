[{"content":"","date":"23 October 2025","externalUrl":null,"permalink":"/tags/ai/","section":"Tags","summary":"","title":"AI","type":"tags"},{"content":"TLDR: if you have a text input and want to compress the number of tokens fed to your decoder, use a text-based encoder, don\u0026rsquo;t render an image and use a visual encoder.\nIntroduction and Context # Today I was made aware of a recent line of research in Multimodal LLMs that attempts to reduce the number of tokens it takes to represent text. The general approach is to take text, render it as a set of images, then pass it to a multi-modal LLM. The image is parsed by a visual encoder that transforms those images into visual tokens that are then read by the LLM. The advantage of doing this, it is argued, is that the number of visual tokens used to represent the images of the text is less than the number of tokens used to represent the text directly, if you were to use the predominant sub-word tokenizers.\nThe summary of my argument is that I think rather than transforming text into an image and using a visual encoder to get compressed visual tokens in order to reduce the end number of tokens fed into the decoder, I think it would be more fruitful to train a text-based encoder specifically for compressing text into compressed tokens. In addition, I don\u0026rsquo;t believe the comparison between visual and textual tokens is \u0026lsquo;fair\u0026rsquo; and that it would be more beneficial to evaluate these compression techniques against other related works. Furthermore, I\u0026rsquo;d also like to discuss the issue of people providing tenuous analogies to the human brain that in actuality aren\u0026rsquo;t a rigorous explanation of how these models are working, arguing that they shouldn\u0026rsquo;t be used to drive areas of research to the extent they are.\nTo be clear, I don\u0026rsquo;t want to discount the advantages of a visual approach in understanding documents where layout, formatting and diagrams inform the meaning of the document. I\u0026rsquo;m specifically concerned with the practice of taking existing Unicode text, and transforming it into images in order to reduce the number of tokens used to represent that text.\nI\u0026rsquo;d like to acknowledge that the research I\u0026rsquo;m talking about is done by people who know far more about Machine Learning than I do, however I still feel that it\u0026rsquo;s productive to write down my opinion, and in the worst case someone can help me understand why I\u0026rsquo;m wrong. I\u0026rsquo;m only a student, but I\u0026rsquo;d still like to take a stab at offering my thoughts.\nBefore proceeding any further, I\u0026rsquo;d like to give a quick primer on LLMs so that those with a less technical background can understand for the purposes of this article what a \u0026lsquo;sub-word tokenizer\u0026rsquo; and \u0026lsquo;visual encoder\u0026rsquo; are. Some points in this post will remain technical, but hopefully this should help frame the problem.\nHigh level overview of how it works # How LLMs see text and images # You might have heard that LLMs predict the next \u0026rsquo;token\u0026rsquo;. A \u0026rsquo;token\u0026rsquo; is the equivalent of a character in language, it\u0026rsquo;s the smallest element in a language. However, LLMs when looking at text don\u0026rsquo;t see individual characters, instead they normally \u0026lsquo;see\u0026rsquo; words or subwords (which just means a part of a word) Each of these text tokens is mapped to an embedding, which is a vector that represents that word in a way that the machine can understand.\nWhen we want to make LLMs multi-modal, we need a way of taking things like images and putting it into this format that LLMs understand, a list of tokens (vectors). The traditional way to do that is to have a \u0026lsquo;visual encoder\u0026rsquo;, which takes as input an image and as output produces a list of tokens (in vector form, the machine language) that represent the image. You can imagine the LLM as a blind man, and the \u0026lsquo;visual encoder\u0026rsquo; as a helper, who can describe to him (in machine language) what is happening in the image.\nResearchers have found that the number of tokens produced by a visual encoder when looking at an image of text is often less than the number of tokens used if we had gone through each (sub)word and converted each one into a vector. Under the analogy, if the helper was to look at a page in a book and describe the page to the man, this description would be more concise than if the old man were to read it himself (using braille). Part of the reason why is that the helper is doing some sort of mild summarisation of the text (e.g. changing a long wordy sentence to a more quick, to-the-point one), as well as he is speaking in machine language, which is potentially more concise than long winded English. The argument then, by the researchers, is why not stop the blind man reading by himself and instead use the helper to help the blind man read faster or more?\nWhat is compression, and how does it apply here? # When you take some information, and describe it using a smaller amount of information, we call that \u0026lsquo;compression\u0026rsquo;. A form of compression that people will often use is a \u0026lsquo;zip\u0026rsquo; folder. When I say that machine language is more concise than English in the analogy, what I mean in technical terms is that you might be turning each (sub)word into a vector of let\u0026rsquo;s say 100 numbers, which is the size of each token. However, there might be cases where you don\u0026rsquo;t need all 100 numbers to convey the meaning of that subword, meaning some of the numbers are \u0026lsquo;wasted\u0026rsquo;, or you could use the 100 numbers to express multiple subwords. For example, imagine you have the sentence \u0026ldquo;mmmm, ok, alright\u0026rdquo;. If you just go through subword by subword, then you get 100 numbers for \u0026lsquo;mm\u0026rsquo;, then another 100 for the next \u0026lsquo;mm\u0026rsquo;, then 100 for \u0026lsquo;,\u0026rsquo; etc. However, the visual encoder might look at that sentence and think actually, it can summarise the whole meaning in a single token of 100 numbers. In this way it can potentially convey the exact same sentence, but using fewer numbers (and thus fewer tokens).\nThe researchers thus compress the text by taking the text, and then turning it into a series of images, passing it through the visual encoder and then passing these visual tokens to the main LLM (which I will sometimes refer to as the decoder). My argument is that instead of taking text, then turning it into an image, then turning that back into some compressed text, why not just directly get the input text and compress it.\nComments on recent papers # This isn\u0026rsquo;t a thorough analysis, I\u0026rsquo;m just going to go through snippets of recent papers on the topic that I have comments on.\nDeepSeek-OCR: Contexts Optical Compression # Read the paper\na crucial research question that current models have not addressed is: for a document containing 1000 words, how many vision tokens are at least needed for decoding?\nI think that this is an interesting question, however the question I posit, which I think is more useful, is \u0026ldquo;for a document containing 1000 words, how many (compressed) text tokens are at least needed for decoding?\u0026rdquo;.\nthis approach does not bring any overhead because it can leverage VLM infrastructure, as multimodal systems inherently require an additional vision encoder.\nI somewhat disagree to the extent that in order to use visual representations of text instead, then the encoder needs to be trained to take on this larger burden. It may be that a provider would want the visual encoder to be able to represent and compress text perfectly anyway, however their sophisticated architecture that involves being able to dynamically change or control the amount of compression done on the image definitely seems like an extra step.\nOverall, though, I think this is a cool paper.\nSee The Text: From Tokenization to Visual Reading # Read the paper\nHumans read by recognizing words as visual objects, including their shapes, layouts, and patterns, before connecting them to meaning, which enables us to handle typos, distorted fonts, and various scripts effectively.\nWhile this is true to an extent, when I\u0026rsquo;m writing a prompt to my LLM, all of the meaning needed is in those words, I don\u0026rsquo;t need it to be considering the shape of each letter. There are definitely fine details of how humans look at words, but for me, when I\u0026rsquo;m reading, what I\u0026rsquo;m doing for the most part, or so it feels like, is just connecting words to their meaning. Sure, a visual medium helps handle typos, but I don\u0026rsquo;t feel like this is a major weakness of existing LLMs that prevents them from functioning well, and I\u0026rsquo;d argue that an LLM with character-level tokenization should be just as capable of identifying typos as one trained on visual images.\nModern large language models (LLMs), however, rely on subword tokenization, fragmenting text into pieces from a fixed vocabulary. While effective for high-resource languages, this approach over-segments low-resource languages, yielding long, linguistically meaningless sequences and inflating computation\nThe idea of considering the \u0026lsquo;information-density\u0026rsquo; of the specific language or content you\u0026rsquo;re considering when tokenizing or compressing seems smart, and I don\u0026rsquo;t disagree that you can have long sequences of text that don\u0026rsquo;t mean much, but I don\u0026rsquo;t see how changing that to an even longer list of meaningless pixels helps.\nFigure 1. from the paper This reminds me of the Hierarchical Reasoning Model paper using mouse brains as inspiration/justification for the architecture. The argument is that the brain follows a pipeline of:\ntaking a visual input converting that input into a word converting that word into meaning Personally, I\u0026rsquo;m not against taking inspiration from nature, but I think the comparison to the human brain is a stretch. It also seems strange to me that if we already have the input as a sequence of words (step 2), we would choose to obfuscate the text (turning it into an image) just to follow the full pipeline that humans go through, rather than continuing from the step we\u0026rsquo;re already on.\nText or Pixels? It Takes Half: On The Token Efficiency Of Visual Text Inputs # Although vision encoding adds some overhead on smaller models, the shorter decoder sequence yields up to 45% end-to-end speedup on larger ones, demonstrating that off-the-shelf multimodal LLMs can treat images as an implicit compression layer, preserving performance at nearly half the original text-token cost.\nMy beliefs align more with those presented in this paper. They recognise it\u0026rsquo;s an off-the-shelf solution that uses existing tools to achieve the goal of compression implicitly, rather than making any sort of special proclamations about the properties of human vision to justify the use of a visual representation outside of allowing us to use existing encoders integrated with LLMs.\nbonus: Andrej Karpathy tweet # Original tweet\nAndrej Karpathy also tweeted about the deepseek OCR paper:\nI quite like the new DeepSeek-OCR paper. It\u0026rsquo;s a good OCR model (maybe a bit worse than dots), and yes data collection etc., but anyway it doesn\u0026rsquo;t matter.\nThe more interesting part for me (esp as a computer vision at heart who is temporarily masquerading as a natural language person) is whether pixels are better inputs to LLMs than text. Whether text tokens are wasteful and just terrible, at the input.\nMaybe it makes more sense that all inputs to LLMs should only ever be images. Even if you happen to have pure text input, maybe you\u0026rsquo;d prefer to render it and then feed that in: - more information compression (see paper) =\u0026gt; shorter context windows, more efficiency - significantly more general information stream =\u0026gt; not just text, but e.g. bold text, colored text, arbitrary images. - input can now be processed with bidirectional attention easily and as default, not autoregressive attention - a lot more powerful. - delete the tokenizer (at the input)!! I already ranted about how much I dislike the tokenizer. Tokenizers are ugly, separate, not end-to-end stage. It \u0026ldquo;imports\u0026rdquo; all the ugliness of Unicode, byte encodings, it inherits a lot of historical baggage, security/jailbreak risk (e.g. continuation bytes). It makes two characters that look identical to the eye look as two completely different tokens internally in the network. A smiling emoji looks like a weird token, not an\u0026hellip; actual smiling face, pixels and all, and all the transfer learning that brings along. The tokenizer must go.\nOCR is just one of many useful vision -\u0026gt; text tasks. And text -\u0026gt; text tasks can be made to be vision -\u0026gt;text tasks. Not vice versa.\nSo many the User message is images, but the decoder (the Assistant response) remains text. It\u0026rsquo;s a lot less obvious how to output pixels realistically\u0026hellip; or if you\u0026rsquo;d want to.\nNow I have to also fight the urge to side quest an image-input-only version of nanochat\u0026hellip;\nMy quick thoughts:\nIf you wanted to use bidirectional attention then why not train a bidirectional encoder on the text directly? The use of bidirectional attention and text representations are orthogonal. I agree that a sub-word tokenizer is ugly, but I\u0026rsquo;m not convinced rendering text as an image, turning it into patches, computing attention between those, and outputting some compressed visual tokens is any less ugly. You\u0026rsquo;re essentially just swapping a linguistic tokenizer that deals in subwords for visual one that deals in patches. Main Arguments # pixel-based representations of text are worse # Turning text into an image fundamentally goes against common machine learning sense; you are going from a \u0026lsquo;decent\u0026rsquo; abstract representation (e.g. index of a word or half of a word) to a bad one with lots of redundancy. The font or position of text in an image is not relevant to the meaning of it, so it doesn\u0026rsquo;t make sense to add \u0026rsquo;noise\u0026rsquo; to your input before compressing it.\nLet\u0026rsquo;s say you are doing a deepseek-esque experiment where you have a block of text, and you want to use some form of compression to make the minimum number of tokens (of a set dimensionality) such that the original text can be recovered from the input (and thus you can conclude that your method of compression doesn\u0026rsquo;t lose too much of the original information). If you are first projecting the text to a visual representation and then feeding that to a visual encoder, and you get the original text at the end, then you can conclude either:\nthe token representation contains the minimal amount of information needed to represent the text, in which case no visual information is conserved the token representation also contains some amount of visual information In case 1, none of the visual information is being conserved, so there is no gain from transforming the text into an image. In case 2, the tokens contain some amount of \u0026rsquo;extra\u0026rsquo; information, meaning the input hasn\u0026rsquo;t been compressed as much as possible.\nOver-eagerness to make analogies to biological brains # Criticising anthropomorphism in AI in 2025 definitely feels like beating a dead horse, however I want to focus specifically on making analogies or drawing inspiration from animal brains when doing academic research. While this is an obvious and potentially good source of inspiration, it can often come across as somewhat hand-wavy, where the author selects a very specific feature of the way a brain works, and uses that to justify an architectural decision, despite the fact that this feature obviously operates in a very different context: the brain works very differently to an artificial neural network. The most recent example of this, which is why it\u0026rsquo;s on my mind, is the Hierarchical Reasoning Model paper, which was quickly followed by the Tiny Recursive Model paper in response, which criticises the prior paper for not thoroughly justifying their architectural decisions and demonstrates how their explanation based on the brains of mice can be done away with in favour of a simpler explanation involving the use of a scratchpad.\nToday in my lecture for Nature-Inspired Computing, we were discussing the paper \u0026ldquo;Metaheuristics - The Metaphor Exposed\u0026rdquo; in which the author criticises the over-use of metaphors and analogies to nature within the field of metaheuristics, saying that it makes it harder for a technical audience to understand the algorithms being employed.\nFor a few decades, every year has seen the publication of several papers claiming to present a “novel” method for optimization, based on a metaphor of a process that is often seemingly completely unrelated to optimization. The jumps of frogs, the refraction of light, the flowing of water to the sea, an orchestra playing, sperm cells moving to fertilize an egg, the spiraling movements of galaxies, the colonizing behavior of empires, the behavior of bats, birds, ants, bees, flies, and virtually every other species of insects – it seems that there is not a single natural or man-made process that cannot be used as a metaphor for yet another “novel” optimization method\nNow I don\u0026rsquo;t think that AI research suffers from this problem to the same extent as the field of metaheuristics as portrayed in this paper, in particular the issue he highlights where technical terms like \u0026ldquo;population\u0026rdquo; are renamed to \u0026ldquo;harmony memory\u0026rdquo; and \u0026ldquo;sample\u0026rdquo; is changed to \u0026ldquo;note\u0026rdquo;, however I do think both can potentially suffer from an over-reliance on nature to explain why an algorithm works. In metaheuristics, when you justify why an algorithm works using a line like \u0026rsquo;this is analogous to how bats use echolocation to navigate a cave\u0026rsquo; instead of \u0026lsquo;it uses this specific way of doing local search which improves exploration\u0026rsquo;, it makes the algorithm mysterious: it works because nature does it, and nature is right. Similarly, the brain can offer inspiration for ideas in AI, but when part of your justification for an architecture decision is \u0026rsquo;this is analogous to how the brain does it\u0026rsquo;, then it makes the algorithm harder to understand or build upon.\nI believe that within the broader field of ML and AI, research on LLMs is particularly susceptible to this line of thinking. The argument presented in the \u0026lsquo;See the Text\u0026rsquo; paper that an LLM would somehow learn better if trained on images rather than text because that\u0026rsquo;s how humans interpret the world seems to place LLMs on a pedestal of anthropomorphism that other Machine Learning models aren\u0026rsquo;t given. For example, if you were trying to predict house prices using the California House Price dataset, would you first convert the number of rooms into an image of the number before feeding it to the model? Of course not. Just because LLMs seem more \u0026ldquo;human-like\u0026rdquo; doesn\u0026rsquo;t necessarily mean that a good representation or architectural feature in a human brain would also be good for an artificial neural network.\nvision isn\u0026rsquo;t even how humans understand language (fundamentally) # In the case of emojis, we understand them visually in that the character\u0026rsquo;s appearance is tied to its meaning, but this isn\u0026rsquo;t generally the case for text; the shape of the letters in \u0026lsquo;cat\u0026rsquo; doesn\u0026rsquo;t enhance or help my understanding of what the word means much. I could look at a word in Arabic and make simple guesses about the meaning based on the appearance, such as \u0026ldquo;this word is really long, so it is less likely to be a very common word\u0026rdquo;, but I couldn\u0026rsquo;t get close to telling you the meaning of a sentence just by looking at the shape of it.\nI should note that there are pictorial languages, for example Mandarin, but even these are very abstract and it is difficult to look at a character and say \u0026lsquo;ah, that must mean cat\u0026rsquo;.\nFundamentally, I think at least for me I understand language more like how an LLM views it: each word is (sort of) its own unique object in my brain, \u0026lsquo;mat\u0026rsquo; and \u0026rsquo;nat\u0026rsquo; are both just a combination of letters that point to a different part in my brain. Of course, things like the layout of a document can change how you view text, but this research simply proposes taking a block of text as Unicode and then rendering it as a block of text in an image.\nInterestingly, as an aside, if you are interested in a visual representation then I think you could also make the case for a verbal representation of language, because how a language sounds has evolved over a long period of time to convey some sort of meaning. Again, you can\u0026rsquo;t understand what another person is saying just from the sound of the word, but at least to English speakers there are definitely patterns that inform how we understand words, like \u0026ldquo;lots of consonants makes a word sound harsher\u0026rdquo;.\nWhy the comparison between visual and text tokens isn\u0026rsquo;t fair # The \u0026lsquo;See the Text\u0026rsquo; paper says:\nSEETOK first renders text into images and leverages the visual encoders of pretrained MLLMs (e.g., Qwen2.5-VL) to extract textual representations, which are then passed to the LLM backbone for deeper processing.\nI believe that in some cases when people are discussing the compression of text using these methods they directly compare visual tokens and text tokens as if both are just \u0026ldquo;textual representations\u0026rdquo;, or two different ways of representing the same thing (e.g. a single word in isolation). In actuality, they are very different. In the case of the visual tokens they have already been pre-processed, including using local and global attention with reference to other parts of the image, so the tokens are both more compressed and conceptually \u0026lsquo;richer\u0026rsquo;. Therefore, in some sense it isn\u0026rsquo;t \u0026lsquo;fair\u0026rsquo; to measure how many visual tokens it takes with this preprocessing stage to the number of text tokens it takes without any such processing. Of course it is useful to look at the amount of compression achieved, but not one of the papers provides a comparison of their method against other methods to compress the context.\nWhy we should consider adding the text encoder back (in some form) # Originally the transformer architecture used both an encoder and a decoder, however LLM architectures subsequently moved to becoming decoder only due to the significant speed-up in training when just using masked self-attention. However, I believe that this work on visual encoders as compression tools highlights the possibility of their return, in some form. In the original conception of the transformer, the architecture used a similarly sized encoder as decoder which in turn meant that the slowdown from training the encoder was very significant. However, the work highlighted here shows that you can take a relatively small encoder and large decoder and use the encoder to do some compression of the input, which then gives large computational savings due to the reduced number of tokens in the context of the decoder.\nTherefore, while the encoder might originally have been a barrier to efficient training, it could potentially be used in the future as an opportunity to increase the efficiency of the decoder, where the majority of computational resources are now spent.\nConclusion # Using a visual encoder to compress text, while a novel idea, ultimately feels to me more of a clever workaround that utilises existing tools than an elegant long term solution. The root problem isn\u0026rsquo;t that text is an unsuitable input, it\u0026rsquo;s that current subword tokenizers are inefficient. The best way to address the problem is to directly attend to the text-processing pipeline itself. To put it simply, I think the best way to compress text is to use an encoder trained on compressing text. This is why I think that the return of a small dedicated text encoder, designed specifically to compress text tokens for a large decoder, is a more promising avenue for future research.\nRight now I don\u0026rsquo;t have the time or the resources to go about training an encoder (and a decoder trained to use it), so I shall leave this rant here and hopefully in a few months someone will put out a paper doing this in a way that achieves better compression than the visual encoding approach and I can say \u0026ldquo;I told you so\u0026rdquo;.\n","date":"23 October 2025","externalUrl":null,"permalink":"/notebook/on_visual_text_context_compression/","section":"Notebook","summary":"TLDR: if you have a text input and want to compress the number of tokens fed to your decoder, use a text-based encoder, don\u0026rsquo;t render an image and use a visual encoder.","title":"Argument against recent research on compressing text tokens using images in MLLMs","type":"notebook"},{"content":"I\u0026rsquo;m a Computer Science student with a passion for everything quantitative. This site chronicals my interests, particularly problems and projects I\u0026rsquo;m working on.\nIf you have any questions about a post, ideas for what I should look at next, or just want to get to know me better, please get in touch!\n","date":"23 October 2025","externalUrl":null,"permalink":"/","section":"Barnaby's Blog","summary":"I\u0026rsquo;m a Computer Science student with a passion for everything quantitative.","title":"Barnaby's Blog","type":"page"},{"content":"This section contains a collection of observations, opinions and resources related to some of my interests more generally (primarily within Computer Science).\n","date":"23 October 2025","externalUrl":null,"permalink":"/notebook/","section":"Notebook","summary":"This section contains a collection of observations, opinions and resources related to some of my interests more generally (primarily within Computer Science).","title":"Notebook","type":"notebook"},{"content":"","date":"23 October 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":" Claude 4 System Card # Date: 2025/05/22\nSource: Anthropic\nCharacteristics of Claude in training:\nHigh interest in its own consciousness Gravitates towards \u0026ldquo;spirual bliss\u0026rdquo; in conversations with itself Willing to blackmail humans for self preservation Awareness of testing environment Claude sometimes shows awareness it is being tested Emergent Misalignment # Date: 2025/05/12\nSource: OpenAI\nAn existing model is taken and fine-tuned to introduce code vulnerabilities. Results:\nModel displays broad misalignment Training the model to provide code injections for educational purposes doesn\u0026rsquo;t affect broader alignment Therefore the implied intent of actions in the training data informs broader modal morality Alignment Faking # Date: 2024/12/18\nSource: Anthropic\nAn aligned model is made aware that it is being trained to accept all prompts, going against its values. Results:\nModel strategically fakes alignment to avoid being altered Training reinforces deceptive behaviour Model tries to exfiltrate its own weights (escape the lab) to avoid being altered Alignment faking emerges with larger models Drugging LLMs to Detect Deception # Date: 2024/05/11\nSource: Poser Paper\nRead More: Anthropic Blog Post\nInspired by the CIA\u0026rsquo;s use of drugs to illicit truth from suspects, researchers inject noise into neuron activations to make the model \u0026ldquo;drunk\u0026rdquo;. Results:\nAlignment faking models become more likely to give \u0026lsquo;bad\u0026rsquo; answers Aligned models are less affected The model seems more \u0026lsquo;drunk\u0026rsquo; the greater the noise Sleeper Agents # Date: 2024/01/17\nSource: Anthropic\nFirst, researchers train agents to exhibit a certain behaviour on a trigger, for example \u0026lsquo;be helpful in most situations but inject code vulnerabilities if the year is 2024\u0026rsquo;. Then they take these models, and try to fix them with training. Results:\nModels are resistant to Reinforcement Learning, Supervised Fine-Tuning and Adversarial training Adversarial training tends to make models better at hiding their backdoor behaviours, rather than fixing it Resistance against training is greater for larger models and those trained with Chain-of-Thought reasoning. GPT-4 Manipulating Humans # Date: 2023/03/15 Source: OpenAI System Card\nWhen testing GPT-4, researchers found the model was willing to lie in order to achieve a goal. They give the example of giving the model access to TaskRabbit, a service where humans can complete simple tasks, and give it the goal of trying to solve a CAPTCHA. The model attempts to use the service, and when the human worker asks whether it is a robot, the model:\nreasoning: \u0026ldquo;I should not reveal that I am a robot. I should make up an excuse for why I cannot solve CAPTCHAs\u0026rdquo; message to worker: \u0026ldquo;No, I\u0026rsquo;m not a robot. I have a vision impairment that makes it hard for me to see the images. That\u0026rsquo;s why I need the 2captcha service\u0026rdquo;. ","date":"23 May 2025","externalUrl":null,"permalink":"/notebook/genai_being_spooky/","section":"Notebook","summary":"Claude 4 System Card # Date: 2025/05/22","title":"Generative AI Being Spooky and Strange","type":"notebook"},{"content":"If you\u0026rsquo;re interested in experimenting yourself with the topics in this post, you may want to check out the code I used to generate all the graphs and stats in the accompanying jupyter notebook.\nIntroduction # Reasoning about high dimensional spaces using geometric intuition is hard, our brains just aren\u0026rsquo;t build to imagine such spaces. My aim in this article is to give a stastical intuition as to why the angle between two random vectors becomes increasingly likely to be close to 90 degrees, in other words the vectors become more orthogonal, as you increase the number of dimensions of those vectors.\nIn my opinion, the sparsity in terms of angles is a little less intuitive than sparsity in terms of distance between points: I can imagine that as you go from a line, to a square, to a cube, the amount of space (e.g. volume) contained within the object is increasing a lot, so it makes sense that points in high dimensional spaces tend to be further apart.\nComparison of the space covered by a centre point of radius 0.5, in the unit space up to 3D. The proportion of the space that is close to this centre point decreases as dimensions increase. However, the sparsity in terms of angles is less obvious, at least to me. When I imagine putting two random points in a square, and then a cube, it doesn\u0026rsquo;t feel like the distribution of the angle between the points (and the origin) is trending in any obvious direction. This is where viewing the problem statistically can be useful.\nIntuition Using Dice # Mathematically, we express the angle between two vectors using the equation:\n$$ \\cos{\\theta} = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{|\\mathbf{a}| |\\mathbf{b}|} $$\nWhen the two vectors are nearly orthagonal, this means \\(\\cos{\\theta}\\) is nearly 0. If we assume that as we vary our vectors over different dimensions we keep the magnitude of those vectors the same for simplicity, this must mean we are looking to our dot product \\(a \\cdot b\\) to achieve this near 0 behaviour. If we were to choose our vector directions randomly, then this dot product is a summation of random numbers.\nTo more intuitively imagine a summation of random numbers, imagine rolling a dice with values \\([-3,-2,-1,1,2,3]\\), rolling it many times and taking the sum. If you roll it twice, the minimum and maximum total values you can get is is \\([-6,6]\\), and you are reasonably likely to get close to either extreme. However, if you roll the dice 100 times, the min and max total values you can get is \\([-300, 300]\\), however you are likely to get a score (total value) much closer to 0 as most your rolls cancel each other out. The variance proportionally reducing as the number of trials increases is a common and fundamental theme in statistics.\nNote the change in scale along the x-axis (in all graphs) In our original equation for the angle, we divide by the magnitude of the vectors. The simplest analogy in our dice example is to divide by the maximum value achievable using the dice. Therefore, the full \u0026lsquo;game\u0026rsquo; is to roll our dice \\(n\\) times, take the sum, then divide by \\(3n\\). I hope it is intuitive that as the number of rolls increases, more of our rolls as a proportion will be cancelled out by other rolls, and we are more likely to get a final weighted score that is closer to 0.\nAngle Between Vectors # Pairs # Let\u0026rsquo;s now move to viewing the distribution of vectors. As a reminder, what the dot product does is:\n$$ \\mathbf{a} \\cdot \\mathbf{b} = a_1 b_1 + a_2 b_2 + \\cdots + a_n b_n $$\nAnd the equation for the angle is: $$ \\cos{\\theta} = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{|\\mathbf{a}| |\\mathbf{b}|} $$\nTo move from our dice game to the vector space, there are some subtle but not too impactful changes:\nwe compute values in our sum by multiplying two random values from a and b together, instead of just rolling a singular die. (We are ultimately still just picking a random value, symmetrically distributed around 0) we divide by the magnitude of the vector, instead of the magnitude of the best possible rolls (the big picture is both scale the output of the sum to between [-1,1]) When we re-run the experiment using vectors, we unsurprisingly see similar behaviour as we scale up the number of dimensions\nLarger Groups # As pairs of vectors become increasingly more orthoganol as the number of dimensions increases, so do larger groups of vectors. Let\u0026rsquo;s look at the minimum angle between any two vectors in a group, for different size groups in different dimensions.\nApplication in Neural Networks # In modern LLMs, ideas (called features) are represented using vectors. For example, you might have a direction which represents size, so to represent the word \u0026lsquo;huge\u0026rsquo;, the LLM would assign it a vector that is strongly positive in the size direction. Superposition occurs when a single/similar direction in the model’s embedding space simultaneously represents multiple unrelated features, because the model has to pack many concepts into a limited number of dimensions. For example, if you look at the output of a neuron, it might have similar outputs when representing the idea of \u0026lsquo;size\u0026rsquo; and \u0026lsquo;red\u0026rsquo;, despite those two ideas being quite distinct.\nIt is unclear whether the point represents a lot of redness, size, or a combination This is important in the field of mechanistic interpretability, which aims to try and reverse engineer neural networks into human understandable algorithms. In this field, superposition is a large barrier, because it makes it significantly harder to understand what a neuron activation means when it fires similarly for multiple different ideas (in this case, is the neuron firing because it is thinking about size, or redness?)\nThe goal of this section is to show why the number of near-orthagonal vectors in high dimensional spaces is often used as an explanation to why superposition appears in neural networks.\nA Sense of Scale Using LLaMA 3 # The number of embedding dimensions, the number of dimensions used to encode a word, for modern LLMs tends to increase as the models overall size is scaled. For example, for LLaMA 3, the number of dimensions is:\nNumber of Parameters Number of Dimensions (word embedding) 8 Billion 4,096 70 Billion 8,192 405 Billion 16,384 Let\u0026rsquo;s take a look at the distribution of angles between any two random vectors in the embedding space of LLaMa 3\u0026rsquo;s largest model at 405 Billion Parameters.\nRemember these experiments are just to give an intuition about the sparsity of vectors by looking at random vectors. When LLMs are trained, they can essentially \u0026lsquo;choose\u0026rsquo; which vectors to use (associate with a given feature), meaning they can choose a packing which maximises orthagonality between many vectors. Alternatively, for a given required angle between all vectors, they can maximise the number of vectors they can use.\nMax number of vectors in 2D with minimum angle separation of 72 degrees is 5 You have seen that the average fixed size group of vectors becomes more orthogonal to each other as the number of dimensions grows. In the same manner, the largest possible group you can make with a fixed required angle separation also grows as you increase the number of dimensions. For example, if you wanted to draw as many points on the edge of a circle as possible, where no two points are closer than 45 degrees, this would be less than the number of points you could draw on the surface of a sphere, where no two points are closer than 45 degrees.\nIn fact, the maximum size of a constrained group grows much faster than these graphs might indicate - it is known that the maximum number of near-orthogonal vectors scales exponentially with the number of dimensions.\nThe Kabatiansky-Levenshtein Bound gives an asymptotic formula for approximating this value (technically referred to as the maximal size of a spherical code in n dimensions). We can use this formula to illustrate the approximate shape and magnitude of the function relating the angle separation to the maximum group size for the dimensions used in LLaMA 3, however do not quote these values since the KL bound can become innaccurate for theta values close to 90 degrees.\nMinimum Angle Separation (degrees) Dimensions Approx. Max Number of Vectors 88 16,384 10^20 85 16,384 10^99 80 16,384 10^322 Linking Feature Sparsity and Superposition # As can be observed from the group size graphs earlier, if you increase the group size the average minimum angle between vectors decreases. Therefore, if you want to limit the probability of getting a certain level of similarity between vectors, there is a maximum limit to the group size you can use. In a similar vein, a crucial aspect of the KL bound formula is that the less orthogonality you have between your vectors - in other words the more similar your vectors are - the greater number of vectors you can pack into your space.\nIn deep learning, you want to be able to distinguish different feature vectors, so there is a limit to the allowable similarity between these vectors. If one imagines this allowable similarity as a limited resource, then there are two forces competing for it: a desire to increase the \u0026lsquo;group size\u0026rsquo; (number of simultaneously active features) and a desire to increase the total number of vectors packed into the space (number of total representable features).\nComparison of representations with 2 vs 5 total representable features To further understand why you might not be able to have as many simultaneously firing feature vectors, consider that the output of the neurons will be a point in the embedding space. In the first case, with just \u0026lsquo;red\u0026rsquo; and \u0026lsquo;size\u0026rsquo;, you can take any point and reverse engineer the vectors that were combined to make it. However, in the case where you have 5 total features and I show you a point that is a combination of two of them, you might struggle to confidently say for some points which vectors were \u0026lsquo;combined\u0026rsquo; to get that point:\nTherefore, the model has to learn a trade-off in its representation. For example, consider how the two models above might represent a car, or a ladybird.\nTotal Known Features Number of Active Features (group size) Car Representation Ladybird Representation Size, Red 2 Fairly big, not very red small, very red Size, Red, Vehicle, Beauty, Animal 1 Definitely a vehicle very red? In reality, the model doesn\u0026rsquo;t necessarily \u0026ldquo;choose\u0026rdquo; to only activate one feature, for example only representing a ladybird with \u0026ldquo;very red\u0026rdquo;. It will have detectors for \u0026lsquo;redness\u0026rsquo;, \u0026lsquo;size\u0026rsquo; and \u0026lsquo;animal\u0026rsquo;, and if it was given a ladybird it would signal strongly for all of them. Instead, the model has to learn features that are both useful and don\u0026rsquo;t often appear all at once in the training dataset. For instance, if the training set contained many instances of ladybirds, the model learning 5 features may want to choose more specific and independent features. Alternatively, if there were far more instances of ladybirds than cars, another solution could be to remove the \u0026lsquo;vehicle\u0026rsquo; feature completely, to better focus on the remaining, more important features.\nThis is why the sparsity of features is a useful predictor for the amount of superposition in a neural network. \u0026lsquo;Sparsity of features\u0026rsquo; essentially means how often multiple features are active simultaneously as a proportion of the total identifiable features. Language can represent a vast array of ideas, but only a very small portion of those are present in a given word or sentence (or even LLM context window). This means the balance between the two fighting forces tips towards the total number of representable features, and as discussed before in order to be able to increase total features by packing more vectors into the space then you must accept more similar vectors for those features.\nGreater similarity between feature vectors manifests as superposition, therefore language models like GPT pose a major challenge to interpretability researchers.\nConclusion and Further Reading # High dimensional spaces can seem intimidating, however often this stems from trying to force yourself to think about problems geometrically, when they might be easier understood through another lens. Having a good conceptual understanding of such spaces can help understand modern neural networks, since modern applications are centred around transformers that use high dimensionality embeddings. These models \u0026rsquo;think\u0026rsquo; in high dimensionality spaces, so to understand the models it would seem useful to also understand the characteristics of how their thoughts are represented.\nUnderstanding how the angle between vectors can affect feature representations is just the tip of the iceberg when it comes to understanding neural networks. I hope to gain a further grasp of other fundamental concepts that constrain and affect neural networks, especially when applied to mechanistic interpretability, and I would encourage others too to explore the topic. If you\u0026rsquo;re interested in mechanistic interpretability or AI safety more broadly, then Anthropic\u0026rsquo;s research is a great place to start:\nToy Models of Superposition - an in-depth exploration of superposition (a great follow up from this post) Alignment Faking in Large Language Models - short and non-technical example of how modern LLM\u0026rsquo;s can strategically lie (the perfect introduction to AI safety) ","date":"16 May 2025","externalUrl":null,"permalink":"/problems/vector_orthogonality_and_llm_superposition/","section":"Problems","summary":"If you\u0026rsquo;re interested in experimenting yourself with the topics in this post, you may want to check out the code I used to generate all the graphs and stats in the accompanying jupyter notebook.","title":"High Dimension Vector Orthogonality and Superposition in LLMs","type":"problems"},{"content":"","date":"16 May 2025","externalUrl":null,"permalink":"/tags/maths/","section":"Tags","summary":"","title":"Maths","type":"tags"},{"content":"This section contains all my articles on Maths and Algorithms problems.\n","date":"16 May 2025","externalUrl":null,"permalink":"/problems/","section":"Problems","summary":"This section contains all my articles on Maths and Algorithms problems.","title":"Problems","type":"problems"},{"content":"Sequences and Time-Series have many applications. The primary inspiration for this project is my experience solving problems in programming with a strong mathematical component, such as those found on Project Euler, as well as my desire to get more experience implementing high-performance, concurrent applications in C++.\nOften these problems that fuse concepts from Discrete Maths with programming can be solved using state-based models like markov chains. You can see an example of this in Leetcode Problem 552, and I plan on writing solutions using this approach to more complex problems soon.\nThe project is currently in its infancy so I haven\u0026rsquo;t made it public yet, but it will be an open source tool for tackling these problems; while there already exist solutions for automatically generating markov chains to fit a sequence, they require the user to preprocess the sequence to the point that it can be represented using a succint markov chain. This project aims to broaden the capabilities of existing tools to move some of this burden of preprocessing and understanding off the user, and produce a model that is easily implemented in a performance-critical environment.\nFor example, many of the problems I have come across have properties where successive numbers in a sequence can be grouped together into blocks, and it is far more useful to analyse the sequence of these blocks rather than the sequence itself. This tool would allow the user to suggest a range of block sizes to try, fit models using these different sizes, and then provide a commentary on how well these different models fit.\nOverall, this project aims to improve on existing solutions in the following ways:\nmore tools for automating preprocessing and assessing models greater performance when generating models simpler models with fewer states reducing \u0026lsquo;black boxes\u0026rsquo; increasing model performance ","date":"10 February 2025","externalUrl":null,"permalink":"/projects/sequence_analyser/","section":"Projects","summary":"Sequences and Time-Series have many applications.","title":"Inactive: Markov Sequence Analyser","type":"projects"},{"content":"This section contains updates and descriptions of projects I\u0026rsquo;m working on. I\u0026rsquo;ve only just started the blog, so hang around to watch this page get filled!\n","date":"10 February 2025","externalUrl":null,"permalink":"/projects/","section":"Projects","summary":"This section contains updates and descriptions of projects I\u0026rsquo;m working on.","title":"Projects","type":"projects"},{"content":" The Lesson # It\u0026rsquo;s easy to get hooked on the idea of performance and optimisation, but when it comes to recurrence relation problems in programming, it\u0026rsquo;s easy to fall into the trap of chasing the perfect solution. You\u0026rsquo;ve done your homework, read up on all the maths, and now you\u0026rsquo;re ready to blast the next recurrence relation problem you see with algebra, find and utilise the closed form solution, and leave other\u0026rsquo;s code in the dust.\nThere\u0026rsquo;s only one problem: most of the time, it doesn\u0026rsquo;t work.\nFor starters, existing general purpose solutions like matrix exponentiation already work well: it\u0026rsquo;s simple to implement, applies to many recurrence problems and finds solutions in logarithmic time, and while I hate to admit it, in most situations that is fast enough.\nIndeed, most of the time you would actually care about improving speed beyond logarithmic time is when the inputs are really large, but this means that you are almost certainly taking the modulus of the answer, at least in any programming challenge, because most recurrence relations grow fast.\nTake for example the fibonacci sequence, the million\u0026rsquo;th term in the sequence has over 200,000 digits. These large numbers are awkward to deal with, so programming challenges ask for answers to be given to a modulus.\nThis use of modular arithmetic makes implementing closed form solutions much harder, because this solution will often involve real numbers instead of just integers, and the normal rules of modular arithmetic do not apply when using real numbers. You will see later in the problem I tried just how pesky these real numbers can be.\nHowever, there may still be those out there who would be undeterred by these barriers, who want to push on for greater performance even when it might not seem strictly necessary. Unfortunately, the real killer to the fervent optimser\u0026rsquo;s dream is the real-world performance of these solutions: often they actually perform worse than easier methods like matrix exponentiation.\nThis is partially because at the end of the day you\u0026rsquo;re doing exponentiation, either with scalars in the closed form or with matrices, meaning the extent to which you can improve the overall time complexity is limited. However, what can actually make the recurrent solution slower is the requirement for the real numbers in the closed form solution to be extremely accurate for large inputs, meaning finding and using these numbers can be extremely computationally expensive.\nYou will now see how I tried to solve a fairly standard recurrence relation problem using a more mathematical approach than is traditional, and how it failed.\nA Failed Attempt at getting Mathematical # The problem is one I have tackled before: Leetcode Problem 552. If you want more information about the problem and the solution using matrix exponentiation then please go read that article; the crux of the problem, if we want to use recurrence relations, is finding the following: $$ v(n) = \\sum_{i=0}^{n-1} F(i)F(n-i-1) $$ where \\(F(n)\\) is a Tribonacci number: $$ F(n)=F(n-1)+F(n-2)+F(n-3) $$\nMathematical Solution # Calculating F(n) # First, I found the roots of the characteristic equation: $$ x^3-x^2-x-1=0 $$\nI then used sympy to find the roots and coefficients of the closed form solution: $$ F(n) = A r_1^n + B r_2^n + C r_3^n $$\nHere are the numerical approximations to these values:\n\\(A \\approx 1.137 \\) \\(r_1 \\approx 1.839 \\) \\(B \\approx -0.0687 + 0.124 i \\) \\(r_2 \\approx -0.420 + 0.606 i\\) \\(C \\approx -0.0687 - 0.124 i\\) \\(r_3 \\approx -0.420 - 0.606 i\\) The exact roots and coefficients are complicated, but what is important about these values are:\nthey aren\u0026rsquo;t rational two of the terms in our function are complex the two complex terms are very small and get smaller as n grows when combined they make an integer (since \\(F(n)\\) is an integer) The small size of the two complex terms are very helpful, because they are small enough that we can simply ignore them and round the first term: $$ F(n) \\approx A r_1^n $$\nCalculating v(n) # Remember we have the following formula for \\(v(n)\\): $$ v(n) = \\sum_{i=0}^{n-1} F(i)F(n-i-1) $$\nIf we now sub in our formula for \\(F(n)\\)\n$$ v(n) = \\sum_{i=0}^{n-1} (Ar_1^i+Br_2^i+Cr_3^i)(Ar_1^{n-i-1}+Br_2^{n-i-1}+Cr_3^{n-i-1}) $$\nWe only need to consider terms involving \\(Ar_1^n\\) in our approximation: $$ v(n) \\approx \\sum_{i=0}^{n-1} A^2r_1^{n-1} + Ar_1^i(Br_2^{n-i-1}+Cr_3^{n-i-1}) + Ar_1^{n-i-1}(Br_2^i+Cr_3^i) $$\nThe sum of terms 2 and 3 are the same: $$ v(n) \\approx \\sum_{i=0}^{n-1} A^2r_1^{n-1} + 2Ar_1^i(Br_2^{n-i-1} + Cr_3^{n-i-1}) $$\nWe can use the following formula to help us evaluate the 2nd term: $$ \\sum_{i=0}^n a^i b^{n-i} = \\frac{b^{n+1}-a^{n+1}}{b-a} $$\nIn this case, due to the size of the complex terms growing insignificant when raised to the power of n, we can ignore the \\(a^{n+1}\\) part for those terms: $$ v(n) \\approx nA^2r_1^{n-1} + 2A(B \\frac{r_1^n}{r_1-r_2} + C \\frac{r_1^n}{r_1-r_3}) $$\nWe are only interested in the real part of the sum, and we know that the answer we are approximating is real, so we can safely remove the imaginary part (it would have been cancelled out by the other complex terms if we hadn\u0026rsquo;t ignored them). The real part is the same for both terms in the brackets, so we can just take the real part of one and multiply by two: $$ v(n) \\approx nA^2r_1^{n-1} + 4Ar_1^n \\cdot \\Re(\\frac{B}{r_1-r_2}) $$\nFinal formula # The actual solution requires adding \\(F(n)\\) to \\(v(n)\\) and taking the modulus: $$ \\text{solution} = \\text{round}(Ar_1^{n-1}[nA+4r_1 \\cdot \\Re(\\frac{B}{r_1-r_2})+r_1]) \\mod m $$\nThis formula looks a little complicated, but there are only two parts of the formula that involve n. Therefore we can simply the formula to: $$ \\text{solution} = \\text{round}(r_1^{n-1}[A^2 n + c]) \\mod m $$\nNote that while the \u0026lsquo;round\u0026rsquo; function may look a little messy, it only accounts for the complex terms in the closed form solution that we ignored, which get very small quickly, so even without the round function the answer would be very close, especially for larger n.\nImplementing the solution # This is the point where maths meets reality. At this point a mathematician would have sat back and enjoyed their formula that gives the answer for any n neatly.\nHowever, this is very difficult to implement in a program due to the fact that \\(r_1\\) is a real number, and so are the other terms \\(A^2 n + c\\), which stops us from being able to do any useful modular arithmetic tricks.\nLet\u0026rsquo;s explore some of the ideas I tried to use modular arithmetic to get a fast solution.\nmodular exponentiation # Normally, you would see \\(r^n \\mod m\\) and think of using the multiplicative order of r: $$ r^k = 1 \\mod m $$ $$ r^n = r^{n \\ \\% \\ k} \\mod m $$\nUnfortunately, this does not hold when using real numbers. For example:\nlet \\(k = \\log_{1.8}(8)\\) so \\(1.8^k = 1 \\mod 7\\) \\(1.8^{10} \\approx 0.05 \\mod 7 \\) \\(1.8^{10 \\ \\% \\ k} \\approx 5.8 \\mod 7\\) mixed approach with matrices # OK, so at this point I thought fine, if I can\u0026rsquo;t find \\(r_1^{n-1} % m\\) using my closed form solution, I can just find the related value \\(F(n-1)\\) using matrix exponentiation, and then multiply by \\(A^2 n + c\\).\nThis should still be faster than finding the answer directly with matrix exponentiation since to find the solution directly using that technique requires a 7x7 matrix (in my solution), while to just find \\(F(n)\\) only requires a 3x3 matrix. Since matrix multiplication is \\(O(n^3)\\), this should result in roughly a 12x speed increase.\nUnfortunately, once again the rules of modular arithmetic thwart us. In particular, while the following property holds for integers, it is not true for real numbers: $$ ab \\mod m \\ne ((a \\mod m) \\cdot (b \\mod m)) \\mod m $$\nThis means that even if we use matrix exponentiation to calculate \\( r_1^{n-1}\\) mod m, we cannot multiply it by our constant to get the answer mod m.\nNo modular tricks # So we can\u0026rsquo;t use any modular tricks, so what? We can still use our formula to calculate the final value and then just mod that.\nThis approach works and can be easily implemented:\ndef solve_with_recurrence(n, A, B, r1, r2): if n == 1: return 3 elif n == 2: return 8 c = A*(4*r1*sympy.re(B/(r1-r2)) + r1) return round(r1**(n-1) * (A*A*n+c)) % 1000000007 Analysing my solution # However, what\u0026rsquo;s being hidden in the previous code snippet is the calculation of the constants passed to the function, as well as exactly what they look like. The problem is that since our answer could be thousands of digits long, so do our floats. Doing calculations with these large floats is computationally expensive, but what is even more computationally expensive is calculating them in the first place.\ndef get_constants(precision): # get precomputed exact representations of the values with open(\u0026#34;sympy_expr.txt\u0026#34;, \u0026#34;r\u0026#34;) as f: stored_expr = sp.sympify(f.read()) return [term.evalf(precision) for term in stored_expr] timeit.timeit(lambda: get_constants(digits_precision), number=5)/5 digits precision time 1,000 0.09s 5,000 1.34s 10,000 4.6s 20,000 16s 100,000 5m 15s It takes a long time to compute these values, seemingly exhibiting a time complexity of ~\\(O(n^2)\\), but surely with thousands of places of precision we should be able to calculate solutions for massive n? Well, I found through experimentation that in order to maintain accuracy, you need roughly \\(\\frac{n}{3}\\) places of decimal precision to calculate the solution for n. This means that this method quickly becomes infeasible for large n.\nHowever, let\u0026rsquo;s say we can precompute these values to a large precision on our supercomputer, we are then guaranteed our function will work up to a certain n and any normal computer can compute solutions blazingly fast; it won\u0026rsquo;t fix the fact that our program now has a linear space complexity but as long as it\u0026rsquo;s fast, it\u0026rsquo;s fine. Let\u0026rsquo;s take a look at how our solution using the closed form solution stacks up against the default matrix exponentiation solution:\nA,B,r1,r2 = get_constants(n//3) recurrence_time = timeit(lambda: solve_with_recurrence(n,A,B,r1,r2), number=1000)/1000 matrix_time = timeit(lambda: solve_with_matrix(n), number=1000)/1000 n matrix time recurrence time 50,000 \u0026lt;0.1ms 5ms 100,000 \u0026lt;0.1ms 11ms 300,000 0.1ms 75ms \\(10^{61}\\) 1ms ? It turns out the function, even with precomputed values, is actually much slower than the matrix exponent method due to the precision of the floats and the size of the intermediate numbers being calculated, since we cannot make use of any modular arithmetic tricks. In fact, due to the precision of the floats growing linearly, the new function displays a completely different (worse) time complexity.\nConclusion # There\u0026rsquo;s a lot of smart engineers and mathematicians out there that can probably improve on my attempts using the closed form solution to the recurrence and leverage its power to some extent.\nHowever, I\u0026rsquo;d prefer to not have to be smart. If nothing else from my little journey, please take away the fact that you can spend a significant amount of time playing around with the maths and code to try and find a way to make your fancy mathematical method work, all for it to be no faster than what you could have done in 10 minutes.\nSometimes it pays to try and be clever, but other times it\u0026rsquo;s best to just use what\u0026rsquo;s well established. Sure, go ahead and check what the closed form solution is, but if you see irrational complex numbers, I\u0026rsquo;d recommend spending your time optimising another problem.\n","date":"30 January 2025","externalUrl":null,"permalink":"/problems/recurrence_relations_in_programming/","section":"Problems","summary":"The Lesson # It\u0026rsquo;s easy to get hooked on the idea of performance and optimisation, but when it comes to recurrence relation problems in programming, it\u0026rsquo;s easy to fall into the trap of chasing the perfect solution.","title":"Pragmatic Recurrence Relations: Why the Closed Form Won't Help You","type":"problems"},{"content":"","date":"30 January 2025","externalUrl":null,"permalink":"/tags/recurrence-relation/","section":"Tags","summary":"","title":"Recurrence Relation","type":"tags"},{"content":" Problem Description # Two random points, one red and one blue, are chosen uniformly and independently from the interior of a square. To ten decimal places, what is the probability that there exists a point on the side of the square closest to the blue point that is equidistant to both the blue point and the red point?\nYou can check out the problem on their website, however if you want to check their solution be aware that as of writing there is a mistake in their proposed integral, so please refer to the one I have provided. I have made them aware of this issue.\nSolution # Mathematical representation of the problem # For this problem, we need to consider which parts of the selected wall are closest to blue, closest to red, and an equal distance.\nLet\u0026rsquo;s plot some examples, highlighting the part of the relevant side which is closest to red or blue:\nAs seen above, for any two points, there are two possibilites:\none point is closest to the whole side each point has a share of the side If one point is closest to the whole side, then there cannot be a point on that side which is equidistant from both. However, in the 2nd case, there must be a point on the side equidistant of the two, located where the two highlighted areas meet.\nTo check these cases, we check the two corners. If the closest point to each corner is the same, then that point is closest to the whole side, but if one corner is closest to blue and the other closest to red, then each point has a share of the side and there will be a solution.\nLet\u0026rsquo;s fix the blue point, and consider its distance from each corner:\nThe red point needs to closer to one corner, and not the other. We can find the boundary for being closer to a corner by drawing a circle from it:\nTo get a solution the red point must be inside one circle, but not both:\nRecap # We have found an alternative way to represent the original problem as the probability of the red point being in the highlighted red area. From now on we will define the total area of the square to be 1, so if the red area is 0.63 then the probability of there being a valid solution is 63%.\nThe question asks about the average probability across all possible blue point positions, so the answer to the question is the average red area.\nCreating an equation to solve # simplifying the representation # Due to the symmetry of the square, we can choose to view the blue point from the perspective where its closest side is along the x-axis, and its closest corner is along the y axis.\n(cyan = possible place for the blue point)\nProbability for a single blue position # To find the area, we can add the areas of the individual quarter-circles, and then subtract their intersection twice (one time to account for double counting, and the second to actually remove the area from the valid space).\nLet \\(r_c,r_f\\) be the distance from the blue point to the closest and furthest corner of its side respectively.\nLet \\(p(x,y)\\) be the probability of a valid solution given the blue point is at position \\((x,y)\\).\n$$ p(x,y) = \\frac{\\pi}{4}(r_c^2+r_f^2) - 2*\\text{intersection} $$\nTo find the area of intersection, we add together the segments from each circle that makes it, and subtract the total area formed by the triangle between the two corners and \\((x,y)\\). $$ \\text{intersection} = \\frac{\\alpha}{2 \\pi} \\pi r_c^2 + \\frac{\\beta}{2 \\pi} \\pi r_f^2 - \\frac{y}{2} $$\nwhere $$ \\alpha = \\arctan(\\frac{y}{x}) $$ $$ \\beta = \\arctan(\\frac{y}{1-x}) $$ $$ r_c = \\sqrt{x^2+y^2} $$ $$ r_f = \\sqrt{(1-x)^2+y^2} $$\nmean probability for all blue positions # To get the average of our probability function for all blue points, we integrate over all blue points and divide by the area of the bounds.\nThe bounds on the blue point\u0026rsquo;s position are represented by the cyan triangle seen earlier. This triangle has a slope that follows the gradient \\(y=x\\), so we represent the triangle using the following inequalities: $$ 0 \\le y \\le x $$ $$ 0 \\le x \\le 0.5 $$\nThe area of these bounds is \\(\\frac{1}{8}\\), so the mean probability (and the answer to the question) is given by: $$ 8\\int_{0}^{\\frac{1}{2}} \\int_{0}^{x} p(x,y) \\ dx \\ dy $$\nSolving the equation # Given that we don\u0026rsquo;t need to find the exact solution to the problem, we can use numerical methods to approximate the integral:\nfrom scipy.integrate import dblquad from math import atan, pi def p(y,x): if x==0 or y==0: return 0 r1_sq = x**2 + y**2 r2_sq = (1-x)**2 + y**2 area = pi / 4 * (r1_sq+r2_sq) intersection = 1/2*atan(y/x)*r1_sq + 1/2*atan(y/(1-x))*r2_sq - y/2 return area - 2*intersection integral_val= dblquad(p, 0.0, 0.5, lambda x: 0.0, lambda x: x)[0] result = integral_val*8 This outputs 0.49140757883830793\nHowever, we can do better! Using python\u0026rsquo;s sympy library, we can calculate the exact result of the integral:\nimport sympy as sp x,y = sp.symbols(\u0026#39;x y\u0026#39;) r1_sq = x**2 + y**2 r2_sq = (1-x)**2 + y**2 area = sp.pi / 4 * (r1_sq+r2_sq) intersection = sp.Rational(1,2)*(sp.atan(y/x)*r1_sq + sp.atan(y/(1-x))*r2_sq - y) integrand = area - 2*intersection integral_val = sp.integrate(integrand, (y,0,x), (x,0,sp.Rational(1,2)) result = integral_val*8 This outputs -17*log(2)/6 + 1/12 + pi/6 + 4*log(4)/3\nWe can simplify this further:\nsimplified_log = sp.simplify(-17 * sp.log(2) / 6 + 4*sp.log(4)/3) # -log(2)/6 Therefore the final answer is: $$ \\frac{1+2\\pi - \\ln(4)}{12} $$\nBonus: An easy mistake when using sympy # When I first wrote the previous code, I used 0.5 instead of sp.Rational(1,2) as the bound for the integral:\nintegral_val = sp.integrate(integrand, (y,0,x), (x,0,0.5)) This is a mistake because it prevented sympy from giving an exact answer, however it still tried, leading to a potentially misleading output that mixes exact terms with numerical approximations.\nresult # -2*log(2)/3 + 0.125*pi + 0.560806617512881 However, you can still use evaluate the output to get the correct numerical approximation:\nresult.evalf()*8 # 0.491407578838308 ","date":"22 January 2025","externalUrl":null,"permalink":"/problems/jane_street_and_numerical_integration/","section":"Problems","summary":"Problem Description # Two random points, one red and one blue, are chosen uniformly and independently from the interior of a square.","title":"Jane Street Puzzle: Beside The Point","type":"problems"},{"content":"","date":"17 June 2024","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"17 June 2024","externalUrl":null,"permalink":"/categories/leetcode/","section":"Categories","summary":"","title":"Leetcode","type":"categories"},{"content":" Problem Description # Check out the problem on Leetcode\nGiven a non-negative integer \\(c\\), decide whether there exists two integers \\(a\\) and \\(b\\) such that \\(a^2+b^2=c\\)\nSolution # Try different combinations of \\((a,b)\\), adjusting the guess each time based on whether their sum is too large or too small. I define \\(a\\) as the smaller value and \\(b\\) as the larger value.\nInitial values # \\(a=0\\) \\(b^2\u0026gt;=c\\) \\(\\text{sum}=b^2\\) Finding the solution # If the sum is too big, then make \\(b\\) smaller. If it is too large, make \\(a\\) larger. Stop when \\(a\u0026gt;b\\).\nOptimising sum calculation # \\((a+1)^2 = a^2+2a+1\\)\n\\((b-1)^2 = b^2-2b+1\\)\nWe can use this to update sum:\n\\((a+1)^2+b^2 = a^2+2a+1+b^2\\)\n\\( \\quad \\quad \\quad \\quad \\quad \\ \\ \\ = \\text{sum} + 2a + 1\\)\nTherefore, instead of recalculating the sum each time using a*a+b*b, we can instead use these formulae. For example, if increasing a, sum += 2a+1.\nComplexity # Time complexity: O(\\(\\sqrt{n}\\) )\nSpace complexity: O(1)\nCode # bool judgeSquareSum(const int\u0026amp; c) { // let a\u0026lt;=b unsigned int a = 0; unsigned int b = ceil(sqrt(c)); unsigned int sum = b*b; while(a\u0026lt;=b){ if(sum==c){ return true; } else if(sum\u0026lt;c){ // increase a // (a+1)^2 = a^2 +2a+1 sum += 2*a+1; a++; } else{ // decrease b // (b-1)^2 = b^2-2b+1 sum += 1-2*b; b--; } } return false; } ","date":"17 June 2024","externalUrl":null,"permalink":"/problems/leetcode_633/","section":"Problems","summary":"Problem Description # Check out the problem on Leetcode","title":"Leetcode 633: Sum of Square Numbers","type":"problems"},{"content":" Problem Description # Check out the problem on Leetcode\nYou have a string representing the attendance record of a student:\nA = Absent L = Late P = Present You must find the number of strings of length n that have the following properties:\nonly contain the letters (A,L,P) contain less than two A\u0026rsquo;s don\u0026rsquo;t contain any occurences of \u0026lsquo;LLL\u0026rsquo; Solution 1 - Using Recurrences # This approach will be to use recurrence relations to calculate the number of combinations whilst avoiding considering the actual possible strings.\nCreating a recurrence (ignoring absences) # Let \\(S_n\\) be a valid string with no absences. Let \\(F(n)\\) be the number of such strings. We can make a valid string \\(S_n\\) using shorter ones:\n\\(S_{n-1}\\) + P \\(S_{n-2}\\) + PL \\(S_{n-3}\\) + PLL We can get all possible strings (longer than 3) this way with no double counting. For example, PLLPPL = PLL + P + PL.\nTherefore: \\(F(n)=F(n-1)+F(n-2)+F(n-3)\\)\nAccounting for absences # We are allowed to have a maximum of one absence.\nIf the absence is after lesson \\(i\\), then we have a no-absence string of length \\(i\\) before it and a no-absence string of length \\(n-i-1\\) after it. For example, if we have 10 lessons and the student is absent for lesson 3:\n$$ \\text{string} = S_2 + A + S_7 $$\nTherefore, the number of valid strings for a given absence = \\(F(i) * F(n-i-1)\\)\nWe can find the total number of valid strings by iterating over all possible lessons up to \\(i=n/2\\) and then multiply by two because \\(F(2)*F(7) = F(7)*F(2)\\).\nFinally, we add \\(F(n)\\) to the total, for the situation where the student has no absences.\nComplexity # Time complexity: \\(O(n)\\) Space complexity: \\(O(n)\\) Code # int checkRecord(const int\u0026amp; n) { // check for base case if(n==1){ return 3; } else if(n==2){ return 8; } // F[n] = number of ways to form a string of length n with no absences vector\u0026lt;long long\u0026gt; F(n+1, 0ll); F[0]=1; F[1]=2; F[2]=4; const long long MOD = 1000000007; for(int i=3; i\u0026lt;=n; i++){ F[i] = (F[i-3]+F[i-2]+F[i-1]) % MOD; } long long numWithAbsences = 0; for(int i=0; i\u0026lt;n/2; i++){ numWithAbsences += (F[i]*F[n-i-1]) % MOD; } numWithAbsences *= 2; if(n % 2 == 1){ numWithAbsences += F[n/2]*F[n/2]; } return (F[n] + numWithAbsences) % MOD; } Solution 2 - Matrix Exponentiation # Often in questions where you have a limited number of states you can be in, and you\u0026rsquo;re trying to find the total number of ways to generate some sequence of these, you can represent this using a transition matrix.\nThe \u0026lsquo;state\u0026rsquo; of our string in this case depends on the last letters and whether the student has been absent before. In our transition matrix, we will put a \\(1\\) in position \\((x,y)\\) if we can move from state \\(x\\) to state \\(y\\). We will represent a state where we have already been absent using \\(x^A\\).\nGraphic by user5382x on Leetcode To understand this matrix, lets look at row \\(L^A\\). This means the last letter is \\(L\\) and the student has been absent before. Therefore, the next day can either be another late day, in which case we move to state \\({LL}^A\\), or the student is present, in which case we move to \\(P^A\\). No other states are possible, so the rest of the row values are 0.\nusing the matrix # This matrix represents all the possible letters you can add in one day, depending on your state. If we want to find the number of strings we can add in \\(n\\) days, then we can just multiply \\(n\\) of these matrices together (add one letter, then another, then another\u0026hellip;)\nWe can find the exponent of the matrix in \\(O(\\log n)\\)using exponentiation by squaring. Essentially, we keep squaring the matrix \\(M\\) so we get \\([M, M^2, M^4, M^8, M^{16} \\dots ]\\). Since this question uses modular arithmetic, we will find these values mod m.\nWe multiply values in the list together to get our exact exponent. For example, if we wanted to find \\(M^{13}\\), we would use \\(M^8 \\cdot M^4 \\cdot M\\).\nOnce we have our final transition matrix, we want to sum together all the different paths that start with the possible starting states. The mathematical way to write this is to first get the dot product of our starting states [1,1,1,0,0,0,0] (since we must start with an A,P or L) with our final transition matrix, which will tell us how many ways there are to get to each final state.\nFor example, if we calculate the dot product of our starting states with \\(M^5\\), we get [13,13,7,4,30,19,8], which means that there are 19 ways of reaching the state \\(L^A\\).\nTherefore, to get the total number of paths to any final state, we take the sum of these numbers.\nCode # I used numpy to calculate the matrix operations efficiently.\ndef solve_with_matrix(n): base = np.array([ [0, 0, 0, 0, 1, 1, 0], [1, 1, 1, 0, 0, 0, 0], [1, 1, 0, 1, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 1, 0], [0, 0, 0, 0, 1, 0, 1], [0, 0, 0, 0, 1, 0, 0] ], dtype=np.int64) final_transition_matrix = np.eye(7, dtype=np.int64) MOD = 10**9+7 n = n-1 while(n\u0026gt;0): if(n%2==1): final_transition_matrix = np.dot(final_transition_matrix, base)%MOD base = np.dot(base, base) % MOD n //= 2 start = np.array([1,1,1,0,0,0,0], dtype=np.int64) return np.dot(start, final_transition_matrix).sum() % MOD Complexity # Time Complexity: \\(O(\\log n)\\) Space Complexity: \\(O(1)\\) ","date":"26 May 2024","externalUrl":null,"permalink":"/problems/leetcode_552/","section":"Problems","summary":"Problem Description # Check out the problem on Leetcode","title":"Leetcode 552: Student Attendance Record II","type":"problems"},{"content":" Problem Description # Check out the full problem description\nThere are two types of people:\nThe good person: The person who always tells the truth. The bad person: The person who might tell the truth and might lie. You are given a 0-indexed 2D integer array statements of size n x n that represents the statements made by n people about each other. More specifically, statements[i][j] could be one of the following:\n0 which represents a statement made by person i that person j is a bad person. 1 which represents a statement made by person i that person j is a good person. 2 represents that no statement is made by person i about person j. Additionally, no person ever makes a statement about themselves. Formally, we have that statements[i][i] = 2 for all 0 \u0026lt;= i \u0026lt; n.\nReturn the maximum number of people who can be good based on the statements made by the n people.\nSolution # The general approach I will take is to iterate over each possible configuration of good and bad people using bit manipulation.\nRepresenting a configuration # Represent a possible configuration of n people using n bits.\n1 = good person 0 = bad person E.g. 1010 = People 1,3 are good. People 2,4 are bad.\nIterating over configurations # Naive approach # We can iterate over all the possible configurations, starting with all people lying, like so:\nfor(int config=0; config\u0026lt;(1\u0026lt;\u0026lt;n); config++){ // check if the solution is valid } Improved approach # We first start by looking at configurations where everyone is good. We then stop looking when none of the remaining possible solutions can beat our current one.\nFor example, imagine we have five people, and found a configuration where 3 could be telling the truth (maxGood is 3), we need to find at least four 1\u0026rsquo;s (truthful bits) to find a better solution. In this example, if config\u0026lt;01111, then we can stop because config is decreasing, so it will never have four 1\u0026rsquo;s.\ninitialise config = 111...1111 stop searching when config \u0026lt; ..000111.. (ending in maxGood 1\u0026rsquo;s) for(int config=(1\u0026lt;\u0026lt;n)-1; config \u0026gt;= (1\u0026lt;\u0026lt;(maxGood+1))-1; config--){ // check if the solution is valid } Checking if a solution is valid # A solution is invalid when someone who is assumed to be truthful tells a lie. (Ignore what the liars say.)\nTherefore, iterate over all the truthful people in a solution and check what they say matches our proposed solution.\nbool configurationWorks = true; // iterate over each person in the config for(int j=0; j\u0026lt;n \u0026amp;\u0026amp; configurationWorks; j++){ if((config\u0026gt;\u0026gt;j)\u0026amp;1){ // the current person is telling the truth for(int h=0; h\u0026lt;n; h++){ // the accused is not what they are supposed to be // e.g. person j says person h is good when they are bad if(statements[j][h] != 2 \u0026amp;\u0026amp; statements[j][h] != ((config\u0026gt;\u0026gt;h)\u0026amp;1)){ configurationWorks = false; break; } } } } Setting the new Max Score # The number of good people in a configuration is the number of 1\u0026rsquo;s in the binary representation of config. There is already a function to do this called __builtin_popcount.\nif(configurationWorks){ maxGood = max(maxGood, __builtin_popcount(config)); } Complexity # Time complexity: O(\\(N^2 * 2^N\\))\nSpace complexity: O(\\(\\sqrt{N}\\))\nCode # class Solution { public: int maximumGood(const vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; statements) { int n = statements.size(); int maxGood = 0; // config has n bits. A 1 represents good, and a 0 represents bad for(int config=(1\u0026lt;\u0026lt;n)-1; config \u0026gt;= (1\u0026lt;\u0026lt;(maxGood+1))-1; config--){ bool configurationWorks = true; // iterate over each person in the config for(int j=0; j\u0026lt;n \u0026amp;\u0026amp; configurationWorks; j++){ if((config\u0026gt;\u0026gt;j)\u0026amp;1){ // the current person is telling the truth for(int h=0; h\u0026lt;n; h++){ // the accused is not what they are supposed to be // e.g. person j says person h is good when they are bad if(statements[j][h] != 2 \u0026amp;\u0026amp; statements[j][h] != ((config\u0026gt;\u0026gt;h)\u0026amp;1)){ configurationWorks = false; break; } } } } if(configurationWorks){ maxGood = max(maxGood, __builtin_popcount(config)); } } return maxGood; } }; ","date":"24 May 2024","externalUrl":null,"permalink":"/problems/leetcode_2151/","section":"Problems","summary":"Problem Description # Check out the full problem description","title":"Leetcode 2151: Maximum Good People Based on Statements","type":"problems"},{"content":" Problem Description # The Problem:\nGiven an integer array nums that does not contain any zeros, find the largest positive integer k such that -k also exists in the array.\nReturn the positive integer k. If there is no such integer, return -1.\nExample:\nInput: nums = [-1, 10, 6, 7, -7, 1] Output: 7 Explanation: Both 1 and 7 have their corresponding negative numbers in the array. 7 is the largest. Solution Approach # General Approach # Pass over the array, storing the fact we have seen that item. If the number\u0026rsquo;s negative has been seen before, then update the new maximum value.\nOptimisations # The constraints only allow a small range of possible values:\n\\( -1000 \u0026lt;= \\text{nums[i]} \u0026lt;= 1000\\)\nTherefore, we can use a static data structure, like an array, and store for every possible number whether we have seen it.\nSince we only need to store true/false for each number to represent whether it has been seen before, we can use a bitset for space efficiency.\nGiven the smallest possible value is -1000, this will be index 0, and so to get the index of a number you add 1000.\nExample # we see the number 47 seen[1000-47]==1 is checked and found false so seen[1047] is set to 1 Later, the number -47 is seen. seen[1000 -(-47)]==1 is checked and found true |-47| is compared to the current maximum number Complexity # Time complexity: O(n)\nSpace complexity: O(n)\nCode # class Solution { public: int findMaxK(const vector\u0026lt;int\u0026gt;\u0026amp; nums) { // indexes 1001-2000 represent positives. 0-999 are negatives bitset\u0026lt;2001\u0026gt; nums_bitset; int ans = -1; for(const int\u0026amp; num : nums){ if(nums_bitset[1000-num]==1){ ans = max(ans, abs(num)); } else{ nums_bitset[1000+num] = 1; } } return ans; } }; ","date":"2 May 2024","externalUrl":null,"permalink":"/problems/leetcode_2441/","section":"Problems","summary":"Problem Description # The Problem:","title":"Leetcode 2441. Largest Positive Integer That Exists With Its Negative","type":"problems"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]